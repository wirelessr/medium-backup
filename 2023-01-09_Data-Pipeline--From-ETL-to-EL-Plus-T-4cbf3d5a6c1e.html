<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Data Pipeline: From ETL to EL Plus T</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Data Pipeline: From ETL to EL Plus T</h1>
</header>
<section data-field="subtitle" class="p-summary">
New challenges lead to new tools, then new challenges?
</section>
<section data-field="body" class="e-content">
<section name="9f59" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8446" id="8446" class="graf graf--h3 graf--leading graf--title">Data Pipeline: From ETL to EL Plus T</h3><h4 name="9977" id="9977" class="graf graf--h4 graf-after--h3 graf--subtitle">New challenges lead to new tools, then new challenges?</h4><figure name="f76f" id="f76f" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*bJUifR2TtbI7hmA7" data-width="1000" data-height="667" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*bJUifR2TtbI7hmA7"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@romanenko29061983" data-href="https://unsplash.com/@romanenko29061983" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Roman Pentin</a> on <a href="https://unsplash.com/photos/T5QT2bmiD4E" data-href="https://unsplash.com/photos/T5QT2bmiD4E" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure><p name="68f9" id="68f9" class="graf graf--p graf-after--figure">In the data pipeline, we often talk about ETL, aka Extract-Transform-Load, which is actually a very simple process as follows.</p><figure name="660e" id="660e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*r_MNWhwg7nVKZm-_B-2JtA.png" data-width="1000" data-height="568" src="https://cdn-images-1.medium.com/max/800/1*r_MNWhwg7nVKZm-_B-2JtA.png"><figcaption class="imageCaption">Data flow of ETL</figcaption></figure><p name="c65d" id="c65d" class="graf graf--p graf-after--figure">Given an application extracts data from a source and saves it to another data storage after some processing and conversion, this is what ETL does. This is a complete ETL process, and to be honest, most non-ETL applications are doing something like that.</p><p name="c6d1" id="c6d1" class="graf graf--p graf-after--p">Let me use a simple Python example to demonstrate the implementation of ETL.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="e540" id="e540" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Open the CSV file containing the data</span><br /><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;orders.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> file:<br />  <span class="hljs-comment"># Use the CSV module to read the data</span><br />  reader = csv.DictReader(file)<br /><br />  <span class="hljs-comment"># Iterate over the rows in the CSV file</span><br />  <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> reader:<br />    <span class="hljs-comment"># Extract the data from the row</span><br />    order_id = row[<span class="hljs-string">&quot;order_id&quot;</span>]<br />    customer_name = row[<span class="hljs-string">&quot;customer_name&quot;</span>]<br />    order_date = row[<span class="hljs-string">&quot;order_date&quot;</span>]<br />    order_total = row[<span class="hljs-string">&quot;order_total&quot;</span>]<br />    <span class="hljs-comment"># Insert the data into the database</span><br />    cursor = db.cursor()<br />    cursor.execute(<br />      <span class="hljs-string">&quot;INSERT INTO orders (order_id, customer_name, order_date, order_total) VALUES (%s, %s, %s, %s)&quot;</span>,<br />      (order_id, customer_name, order_date, order_total)<br />    )<br />    db.commit()</span></pre><p name="4b11" id="4b11" class="graf graf--p graf-after--pre">This is a simple ETL example, we extract each line from a CSV file (data source), and convert the order details from CSV format to the corresponding columns in the database, and finally write to the database.</p><p name="4d03" id="4d03" class="graf graf--p graf-after--p">As the database evolves, this process also evolves, and the ETL also has a variation, ELT, which seems to be a sequential exchange of T and L. But behind the scenes, the database has actually become more powerful.</p><p name="ebaa" id="ebaa" class="graf graf--p graf-after--p">Let’s continue with the above example, and see how would ELT be handled?</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="ca0b" id="ca0b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;orders.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> file:<br />  <span class="hljs-comment"># Use the LOAD DATA INFILE statement to load the data into the database</span><br />  cursor = db.cursor()<br />  cursor.execute(<br />    <span class="hljs-string">&quot;LOAD DATA INFILE &#x27;orders.csv&#x27; INTO TABLE orders FIELDS TERMINATED BY &#x27;,&#x27; ENCLOSED BY &#x27;\&quot;&#x27; LINES TERMINATED BY &#x27;\n&#x27; IGNORE 1 ROWS&quot;</span><br />  )<br />  db.commit()</span></pre><p name="45fd" id="45fd" class="graf graf--p graf-after--pre">In this example, we use MySQL’s <code class="markup--code markup--p-code">LOAD DATA</code> command to write a CSV file directly into the database, converting it during the writing process, rather than converting the format first and then writing it to the database.</p><p name="582f" id="582f" class="graf graf--p graf-after--p">In the data pipeline, it is actually various combinations of ETL and ELT, and eventually we will produce a data structure to fit the analysis and utilization.</p><p name="b67a" id="b67a" class="graf graf--p graf-after--p">This process is the same concept in the big data domain, except that the data pipeline of big data multiplies a much larger volume of data and needs to face a more diverse data storage.</p><h3 name="6599" id="6599" class="graf graf--h3 graf-after--p">From ETL to EL plus T</h3><p name="46da" id="46da" class="graf graf--p graf-after--h3">In my previous articles, we have already discussed the implementation of many big data pipeline architectures.</p><ul class="postList"><li name="15a8" id="15a8" class="graf graf--li graf-after--p"><a href="https://betterprogramming.pub/evolutionary-data-infrastructure-4ddce2ec8a7e" data-href="https://betterprogramming.pub/evolutionary-data-infrastructure-4ddce2ec8a7e" class="markup--anchor markup--li-anchor" title="https://betterprogramming.pub/evolutionary-data-infrastructure-4ddce2ec8a7e" rel="noopener" target="_blank">Evolutionary Data Infrastructure</a></li><li name="e578" id="e578" class="graf graf--li graf-after--li"><a href="https://betterprogramming.pub/real-time-data-infra-stack-73c597ed05ee" data-href="https://betterprogramming.pub/real-time-data-infra-stack-73c597ed05ee" class="markup--anchor markup--li-anchor" title="https://betterprogramming.pub/real-time-data-infra-stack-73c597ed05ee" rel="noopener" target="_blank">The Infrastructure Stack for Real-Time Data Analysis</a></li></ul><p name="da11" id="da11" class="graf graf--p graf-after--li">For the data source part, we rely heavily on <a href="https://debezium.io/" data-href="https://debezium.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Debezium</a> to capture data changes (CDC).</p><p name="8881" id="8881" class="graf graf--p graf-after--p">The process underlying Debezium is that a group of workers extracts the binlogs of various databases, restores the binlogs to the original data, and identifies the contents of the schema to be passed on to the next data storage.</p><p name="b91b" id="b91b" class="graf graf--p graf-after--p">In other words, each data source must have a group of Debezium workers to be responsible for it.</p><figure name="f8ea" id="f8ea" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*iMSO6zO4Wh1RgowDIeVbgA.png" data-width="816" data-height="670" src="https://cdn-images-1.medium.com/max/800/1*iMSO6zO4Wh1RgowDIeVbgA.png"></figure><p name="e586" id="e586" class="graf graf--p graf-after--figure">When the number of data sources is large, it is a huge effort just to build and maintain the corresponding Debezium. After all, the software does not work properly when left alone, but also requires a complete monitoring and alerting mechanism, which will eventually create a huge operation overhead.</p><p name="3ec7" id="3ec7" class="graf graf--p graf-after--p">In fact, E and L in ETL are the most worthless things in a pipeline. For a pipeline to be effective, the most important thing is to produce effective data based on business logic, and T should be the part we should be focusing on.</p><p name="6e4e" id="6e4e" class="graf graf--p graf-after--p">However, we have to take a lot of time on E and L in the operation of the pipeline, which is not what we expect.</p><p name="8a91" id="8a91" class="graf graf--p graf-after--p">So, is there a solution to this dilemma?</p><p name="f07c" id="f07c" class="graf graf--p graf-after--p">Yes, absolutely, <a href="https://airbyte.com/" data-href="https://airbyte.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Airbyte</a>, and there are many similar solutions, but Airbyte is open source and relatively easy to use.</p><p name="7b47" id="7b47" class="graf graf--p graf-after--p">Let’s take MongoDB to Kafka as an example, as soon as the setup is done it will be like the example below.</p><figure name="0685" id="0685" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*m2lRwue0sdFazt45.png" data-width="2600" data-height="794" src="https://cdn-images-1.medium.com/max/800/0*m2lRwue0sdFazt45.png"></figure><p name="ed4a" id="ed4a" class="graf graf--p graf-after--figure">Airbyte can sync automatically or manually according to the settings, and it can choose incremental sync or full sync. Each sync record is listed, and the sync process can be seen after expanding logs.</p><p name="96d3" id="96d3" class="graf graf--p graf-after--p">As a result, the creation of data pipelines is effectively simplified, and data storage can be integrated smoothly through Airbyte’s Web UI.</p><h3 name="dd95" id="dd95" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="9b87" id="9b87" class="graf graf--p graf-after--h3">This article is not to promote Airbyte, nor is it to introduce the Airbyte architecture in depth, but from the standpoint of an architect, we will find that in the domain of big data, we will face various challenges, and each challenge has a corresponding solution, but also requires high learning costs.</p><p name="6f46" id="6f46" class="graf graf--p graf-after--p">In fact, Airbyte also has its limitations, for example, the synchronization period must specify a regular time interval, such as every hour, even though it can use <a href="https://crontab.guru/" data-href="https://crontab.guru/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">cron representation</a>, but still limited by the fixed time, can not do the real streaming, i.e., once the data is triggered.</p><p name="fb10" id="fb10" class="graf graf--p graf-after--p">In addition, when relying on additional tools, it can be frustrating to encounter this problem.</p><figure name="be95" id="be95" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*bGwrLDPiDy3z6igO.png" data-width="782" data-height="618" src="https://cdn-images-1.medium.com/max/800/0*bGwrLDPiDy3z6igO.png"></figure><p name="7468" id="7468" class="graf graf--p graf-after--figure">Once the problem occurs, where to start to troubleshoot, and how to clarify the scope of the disaster, all need to experience and in-depth learning before there is a way to deal with.</p><p name="6ab0" id="6ab0" class="graf graf--p graf-after--p">Learning these tools is also the biggest challenge for data engineers and data architects, there are many tools to solve many problems. However, big data is really big, and learning all tools is almost impossible.</p><p name="fb31" id="fb31" class="graf graf--p graf-after--p">Finally, let me conclude with a picture.</p><figure name="fc21" id="fc21" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*5i9hq78ueoNhPD1d.jpg" data-width="2270" data-height="1166" src="https://cdn-images-1.medium.com/max/800/0*5i9hq78ueoNhPD1d.jpg"><figcaption class="imageCaption"><a href="https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf" data-href="https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf</a></figcaption></figure><p name="516d" id="516d" class="graf graf--p graf-after--figure graf--trailing">What’s the next?</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/4cbf3d5a6c1e"><time class="dt-published" datetime="2023-01-09T02:12:35.331Z">January 9, 2023</time></a>.</p><p><a href="https://medium.com/@lazypro/data-pipeline-from-etl-to-el-plus-t-4cbf3d5a6c1e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>