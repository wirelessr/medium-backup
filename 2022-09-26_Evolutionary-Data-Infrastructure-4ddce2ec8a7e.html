<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Evolutionary Data Infrastructure</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Evolutionary Data Infrastructure</h1>
</header>
<section data-field="subtitle" class="p-summary">
From Monolith to Self-service Platform
</section>
<section data-field="body" class="e-content">
<section name="ea75" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a02b" id="a02b" class="graf graf--h3 graf--leading graf--title">Evolutionary Data Infrastructure</h3><h4 name="0535" id="0535" class="graf graf--h4 graf-after--h3 graf--subtitle">From Monolith to Self-service Platform</h4><figure name="434a" id="434a" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*-SLCYT8CoQ_oMra8" data-width="1000" data-height="667" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*-SLCYT8CoQ_oMra8"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@mitchel3uo" data-href="https://unsplash.com/@mitchel3uo" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Mitchell Luo</a> on <a href="https://unsplash.com/photos/UiSU4AUcQEA" data-href="https://unsplash.com/photos/UiSU4AUcQEA" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure><p name="c640" id="c640" class="graf graf--p graf-after--figure">All systems start as a small monolith. In the beginning, when resources and manpower are not sufficient, the monolith is the choice we have to make, even the data infrastructure is no exception.</p><p name="6767" id="6767" class="graf graf--p graf-after--p">But as requirements increase, there are more and more scenarios that cannot be achieved by the current architecture, and the system must therefore evolve. Each time the system evolves, it is to solve the problems encountered, so it is necessary to understand the different aspects that need to be considered, and to use the most efficient engineering methods to achieve the goal.</p><p name="c840" id="c840" class="graf graf--p graf-after--p">In this article, we will still start with a monolith, as we have done before.</p><ul class="postList"><li name="be30" id="be30" class="graf graf--li graf-after--p"><a href="https://medium.com/interviewnoodle/shift-from-monolith-to-cqrs-a34bab75617e" data-href="https://medium.com/interviewnoodle/shift-from-monolith-to-cqrs-a34bab75617e" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Shift from Monolith to CQRS</a></li></ul><p name="3233" id="3233" class="graf graf--p graf-after--li">But this time, our goal is not to serve a production environment, but to provide the data infrastructure behind all production environments.</p><p name="7875" id="7875" class="graf graf--p graf-after--p">A data infrastructure is a “place” where all kinds of data are stored, either structured data or time series data or even raw data. The purpose of this big data (and they are really big) is to provide material for data analysis, business intelligence or machine learning.</p><p name="9682" id="9682" class="graf graf--p graf-after--p">In addition to internal uses, there may also be user-facing functions, for example, a list of recommended products on an e-commerce website. The recommendation function is the most commonly used customer scenario for big data, such as recommended products in e-commerce websites and recommended videos in video platforms like Youtube. All of these are the result of analyzing, aggregating, and compiling after storing various user actions.</p><p name="4729" id="4729" class="graf graf--p graf-after--p">After knowing today’s topic, let’s get to work.</p><h3 name="49f8" id="49f8" class="graf graf--h3 graf-after--p">Monolithic architecture</h3><p name="aced" id="aced" class="graf graf--p graf-after--h3">Let’s begin with a monolithic architecture.</p><figure name="0586" id="0586" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fnfQubpK1W0csgLyyeF2SQ.png" data-width="652" data-height="514" src="https://cdn-images-1.medium.com/max/800/1*fnfQubpK1W0csgLyyeF2SQ.png"></figure><p name="7a55" id="7a55" class="graf graf--p graf-after--figure">Whatever the service is, a typical monolithic architecture is a service paired with a database. All data analysis, business intelligence and machine learning run directly on this database.</p><p name="7b35" id="7b35" class="graf graf--p graf-after--p">This is the simplest practice, but it is also the beginning of all products.</p><p name="31e2" id="31e2" class="graf graf--p graf-after--p">Why do we show it in monolithic style? Wouldn’t it be better to do more?</p><p name="25e6" id="25e6" class="graf graf--p graf-after--p">Well, if the product dies before it is popular, then the monolith will be enough to maintain full features. In order to avoid over-engineering and the investment cost getting exploded, we prefer to start with the easiest proof-of-concept monoliths.</p><p name="2b71" id="2b71" class="graf graf--p graf-after--p">Of course, this architecture is sure to encounter problems. There are three common problems.</p><ol class="postList"><li name="46c7" id="46c7" class="graf graf--li graf-after--p">The analysis affects the production performance.</li><li name="bf77" id="bf77" class="graf graf--li graf-after--li">If the microservices are introduced, running analysis on each microservice’s database is suffered.</li><li name="89cd" id="89cd" class="graf graf--li graf-after--li">It is very difficult to perform cross-service analysis.</li></ol><p name="b1e5" id="b1e5" class="graf graf--p graf-after--li">Various analyses inevitably take up a lot of database resources, so customers directly feel the poor performance during that time.</p><p name="86f6" id="86f6" class="graf graf--p graf-after--p">In addition, when product requirements increase, we usually adopt microservices for rapid iteration, which means many service and database pairings are created. In order to be able to analyze various product requirements, it is necessary to build several analysis frameworks on every database.</p><p name="6d00" id="6d00" class="graf graf--p graf-after--p">Moreover, even if each service can be analyzed individually, we cannot easily obtain a global view because the analytics are independent of each other.</p><p name="2009" id="2009" class="graf graf--p graf-after--p">Therefore it is necessary to perform the first evolution.</p><h3 name="c2bd" id="c2bd" class="graf graf--h3 graf-after--p">Batch processing</h3><p name="acd8" id="acd8" class="graf graf--p graf-after--h3">We have two goals, firstly to have independent data storage so it does not directly affect the production environment, and secondly to be able to perform cross-service analysis.</p><figure name="4f82" id="4f82" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_PWM4c8ojy06v3MKlfraOA.png" data-width="1354" data-height="666" src="https://cdn-images-1.medium.com/max/800/1*_PWM4c8ojy06v3MKlfraOA.png"></figure><p name="78e6" id="78e6" class="graf graf--p graf-after--figure">To achieve these goals, we set up a centralized data warehouse so that all data can be stored in one place. All analyses were migrated from the databases of each service to the data warehouse.</p><p name="0dc3" id="0dc3" class="graf graf--p graf-after--p">We also need a batch processing role to collect data and even pre-process data periodically so that the data warehouse contains not only raw data but also structured data to speed up the analysis.</p><p name="955d" id="955d" class="graf graf--p graf-after--p">For batch processing, I recommend using <a href="https://airflow.apache.org/" data-href="https://airflow.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache Airflow</a>, which is easy to manage and easy to script for various DAGs to address the needs of multiple batch processing scenarios.</p><p name="b027" id="b027" class="graf graf--p graf-after--p">Such an architecture is simple but powerful enough to handle a wide range of analysis and reporting scenarios. Nevertheless, there are two drawbacks that cannot be easily solved.</p><ol class="postList"><li name="aec2" id="aec2" class="graf graf--li graf-after--p">lack of real time</li><li name="3f41" id="3f41" class="graf graf--li graf-after--li">lack of schema</li></ol><p name="bcaf" id="bcaf" class="graf graf--p graf-after--li">Due to the periodic batch tasks, we can’t do real-time analysis. If we run the task once an hour, then the data we see in the data warehouse is a snapshot of the previous hour’s data.</p><p name="6bc2" id="6bc2" class="graf graf--p graf-after--p">In addition, batch tasks require knowledge of the data schema of each service in order to get the data correctly and save it to the corresponding warehouse table. Assuming our data warehouse is <a href="https://cloud.google.com/bigquery" data-href="https://cloud.google.com/bigquery" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GCP BigQuery</a>, the schema in the warehouse table also needs to be created and modified manually.</p><p name="3f78" id="3f78" class="graf graf--p graf-after--p">When the number of services is very large, just knowing the schema can take a lot of time, not to mention managing it.</p><p name="8fd1" id="8fd1" class="graf graf--p graf-after--p">So, let’s do a second evolution.</p><h3 name="5a0e" id="5a0e" class="graf graf--h3 graf-after--p">Stream processing</h3><p name="c597" id="c597" class="graf graf--p graf-after--h3">To solve the real-time problem, the most straightforward idea is to use a streaming architecture.</p><figure name="6d34" id="6d34" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MU10qBeAHHTmXSQ_kfG-Vw.png" data-width="1392" data-height="666" src="https://cdn-images-1.medium.com/max/800/1*MU10qBeAHHTmXSQ_kfG-Vw.png"></figure><p name="3d92" id="3d92" class="graf graf--p graf-after--figure">First, all the database changes of the service are captured, then the changes are sent to the stream, and finally the stream is archived to the warehouse.</p><p name="892d" id="892d" class="graf graf--p graf-after--p">But is there a way to solve the schema problem? The answer is, yes, through <a href="https://github.com/confluentinc/kafka-connect-bigquery" data-href="https://github.com/confluentinc/kafka-connect-bigquery" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">KCBQ</a>.</p><p name="a988" id="a988" class="graf graf--p graf-after--p">Let’s take a closer look at the internal architecture of streaming.</p><figure name="5685" id="5685" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9OSSVYgtWGiKqDT0luYXDQ.png" data-width="1392" data-height="568" src="https://cdn-images-1.medium.com/max/800/1*9OSSVYgtWGiKqDT0luYXDQ.png"></figure><p name="fa97" id="fa97" class="graf graf--p graf-after--figure">We use <a href="https://debezium.io/" data-href="https://debezium.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Debezium</a> to capture changes to each database and send the streams to Kafka, and later KCBQ subscribes to the Kafka streams and archives them to BigQuery.</p><p name="bbc2" id="bbc2" class="graf graf--p graf-after--p">It is worth noting that KCBQ can automatically update the internal schema of BigQuery (setting needs to be enabled, and AVRO format must be used).</p><p name="1331" id="1331" class="graf graf--p graf-after--p">At this stage, we have a complete data set for analysis, and we have enough data to process for all kinds of data analysis, both real time and batch. However, in the previous stage, we were able to accelerate the analysis by batching pre-processing to generate structured data, and we still want to retain this capability.</p><p name="58d1" id="58d1" class="graf graf--p graf-after--p">In addition, although the data analysis is working properly, we are currently unable to support external customers to access the data. The main reason is the slow response time of data warehouse, which is a big problem when we need to interact with external customers. For example, if a recommendation list takes 5 minutes to be generated, the effectiveness of the recommendation will be close to zero.</p><p name="0a38" id="0a38" class="graf graf--p graf-after--p">Furthermore, when the number of features to be provided to external customers increases, data warehouse can be difficult to handle. For example, search is a common feature, but the search performance of data warehouse is very poor.</p><p name="1029" id="1029" class="graf graf--p graf-after--p">Therefore, we have to integrate batch processing and streaming processing so that this data infrastructure can flexibly support various customer-oriented functions.</p><h3 name="a733" id="a733" class="graf graf--h3 graf-after--p">Integrated architecture</h3><p name="19de" id="19de" class="graf graf--p graf-after--h3">We already have a batch architecture and a streaming architecture, now let’s integrate them properly.</p><figure name="d270" id="d270" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N_PMvKyt7o28zRWo3kYuwA.png" data-width="1392" data-height="536" src="https://cdn-images-1.medium.com/max/800/1*N_PMvKyt7o28zRWo3kYuwA.png"></figure><p name="3627" id="3627" class="graf graf--p graf-after--figure">We still archive the changes from each database to the data warehouse through streaming, but we periodically use batch processing to pre-process the raw data for data analysis.</p><p name="ef63" id="ef63" class="graf graf--p graf-after--p">At the same time, the streaming framework not only archives, but also aggregates the streams to generate real-time customer-facing data storage. For example, <code class="markup--code markup--p-code">ElasticSearch</code> is often used in search contexts, while the event-sourced view is used to provide customer-facing functions, e.g., recommendation lists.</p><p name="4dd5" id="4dd5" class="graf graf--p graf-after--p">But these functions usually also require fact tables, which can be migrated from data warehouses if they are static, otherwise, they need to be implemented through <a href="https://betterprogramming.pub/design-pattern-of-streaming-enrichment-17a9eb065eca" data-href="https://betterprogramming.pub/design-pattern-of-streaming-enrichment-17a9eb065eca" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">stream enrichment</a>.</p><p name="d86b" id="d86b" class="graf graf--p graf-after--p">So let’s further have a look at the internals of streaming architecture.</p><figure name="b142" id="b142" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fOuIl3-3-pTc9Wxk2MpRXw.png" data-width="1392" data-height="472" src="https://cdn-images-1.medium.com/max/800/1*fOuIl3-3-pTc9Wxk2MpRXw.png"></figure><p name="39a0" id="39a0" class="graf graf--p graf-after--figure">The framework is basically the same as the previous one, but with a new role, <code class="markup--code markup--p-code">StreamAPI</code>.</p><p name="6831" id="6831" class="graf graf--p graf-after--p">There is no specific framework specified in <code class="markup--code markup--p-code">StreamAPI</code>, even if using Kafka’s native library is fine.</p><p name="3dc6" id="3dc6" class="graf graf--p graf-after--p">But if using the stream processing framework can save a lot of implementation efforts, such as data partitioning, fault tolerance, state persistence, and so on.</p><p name="8b4b" id="8b4b" class="graf graf--p graf-after--p">Therefore, I still recommend using a streaming framework such as <a href="https://flink.apache.org/" data-href="https://flink.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache Flink</a> or <a href="https://kafka.apache.org/documentation/streams/" data-href="https://kafka.apache.org/documentation/streams/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache Kafka Streams</a>.</p><p name="65fe" id="65fe" class="graf graf--p graf-after--p">At this point, the entire data infrastructure is fully functional. Whether it is for internal data analysis or for external customers, this framework can meet the requirements.</p><p name="be5a" id="be5a" class="graf graf--p graf-after--p">However, this architecture doesn’t mean that it’s done once and for all.</p><p name="8432" id="8432" class="graf graf--p graf-after--p">The next problem is the management problem. So far, the most common task we need to do is not feature development, but the configuration and operation of so many middleware. For each new service, we need to build <code class="markup--code markup--p-code">Debezium</code>, add Kafka topic, configure the streaming framework, and determine the final <code class="markup--code markup--p-code">sink</code>. This process also applies to a new feature request.</p><p name="7abc" id="7abc" class="graf graf--p graf-after--p">In addition, managing the permissions between all data stores is also a problem. When a new person comes into the organization, each participant in the framework has to grant some permissions. Similarly, when a new service or feature is launched, the permissions between services need to be managed.</p><p name="a7e1" id="a7e1" class="graf graf--p graf-after--p">What’s more, when all the data is dumped into the data warehouse, we have to encrypt or mask the PII (personally identifiable information) to be compliant with legal requirements, such as GDPR.</p><p name="c69e" id="c69e" class="graf graf--p graf-after--p">These tasks rely on a lot of manual work or semi-automatic scripts, which undoubtedly add a heavy load to a busy organization.</p><h3 name="b393" id="b393" class="graf graf--h3 graf-after--p">Automation (Self-service Platform)</h3><p name="f9bb" id="f9bb" class="graf graf--p graf-after--h3">Therefore, it is necessary to establish a fully automated mechanism to solve all management issues of the data infrastructure.</p><figure name="0d6e" id="0d6e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zfx9WYTuaqVWCX9iS7V6zg.png" data-width="1392" data-height="668" src="https://cdn-images-1.medium.com/max/800/1*zfx9WYTuaqVWCX9iS7V6zg.png"></figure><p name="bf29" id="bf29" class="graf graf--p graf-after--figure">Of course, this automation mechanism should ideally be self-service, and the most successful example I have seen is <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a" data-href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Netflix’s Keystone</a>.</p><p name="a8aa" id="a8aa" class="graf graf--p graf-after--p">This automation platform involves very deep technical details, not an article can simply explain, so I only provide the features of this platform should have, as for the implementation details will depend on the practice of each organization.</p><p name="592f" id="592f" class="graf graf--p graf-after--p">At the infrastructure level, in order to automate the establishment of settings and monitoring, it is necessary to achieve fully automated deployment through IaC (Infrastructure as Code).</p><p name="23e1" id="23e1" class="graf graf--p graf-after--p">In order to address the issue of data ownership, there are three permission-related controls that should also be included in the IaC as follows.</p><ul class="postList"><li name="1efb" id="1efb" class="graf graf--li graf-after--p">Identity Access Management, IAM</li><li name="c2b3" id="c2b3" class="graf graf--li graf-after--li">Role Based Access Control, RBAC</li><li name="212e" id="212e" class="graf graf--li graf-after--li">Access Control List, ACL</li></ul><p name="27a0" id="27a0" class="graf graf--p graf-after--li">Privacy issues require an easy-to-use interface that can be configured to determine what data must be masked. Data Loss Prevention, aka DLP, is also a very popular data privacy topic nowadays.</p><p name="228d" id="228d" class="graf graf--p graf-after--p">As you can see from the above description, there are a lot of implementation details to achieve a fully automated management platform, and there are no shortcuts — implementation, implementation and lots of implementation.</p><h3 name="6326" id="6326" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="56cf" id="56cf" class="graf graf--p graf-after--h3">This article is about the evolution of a monolithic architecture into a fully automated and fully functional data platform through continuous problem solving.</p><p name="31e1" id="31e1" class="graf graf--p graf-after--p">However, there are still a lot of detailed technical selections in the evolution process, such as what data storage to use, what batch processing, and what real-time processing are all worthy of attention.</p><p name="9e6a" id="9e6a" class="graf graf--p graf-after--p">Such an evolution requires a lot of human resources and time, so it is unlikely to be achieved in one step. If and only if we encounter a problem that cannot be solved, we will be forced to move to the next stage.</p><p name="2297" id="2297" class="graf graf--p graf-after--p">In fact, even if we achieve an automated platform, the evolution is still not over.</p><p name="da7b" id="da7b" class="graf graf--p graf-after--p">When all kinds of business requirements and all kinds of products and services have to rely on a single data engineer department, then human resources will encounter a bottleneck. How to solve it?</p><p name="a860" id="a860" class="graf graf--p graf-after--p">Since we have a fully automated management platform, we can let each product department maintain its own data warehouse and data pipeline. The data engineer becomes the provider of the platform solution, not the owner of the data.</p><blockquote name="77e0" id="77e0" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">The Data Mesh is born.</em></blockquote><p name="e27f" id="e27f" class="graf graf--p graf-after--blockquote">However, there are many different approaches to the Data Mesh implementation, and there is no one-size-fits-all architecture pattern.</p><p name="1f93" id="1f93" class="graf graf--p graf-after--p graf--trailing">This is all about the evolution of data infrastructure. If you have any special practices, please feel free to share them with me.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/4ddce2ec8a7e"><time class="dt-published" datetime="2022-09-26T01:49:22.933Z">September 26, 2022</time></a>.</p><p><a href="https://medium.com/@lazypro/evolutionary-data-infrastructure-4ddce2ec8a7e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>