<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>做一個務實的RAG (Pragmatic RAG)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">做一個務實的RAG (Pragmatic RAG)</h1>
</header>
<section data-field="subtitle" class="p-summary">
不要跟我五四三，回點有用的話來
</section>
<section data-field="body" class="e-content">
<section name="69bd" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="cdcc" id="cdcc" class="graf graf--h3 graf--leading graf--title">做一個務實的RAG (Pragmatic RAG)</h3><h4 name="a3e1" id="a3e1" class="graf graf--h4 graf-after--h3 graf--subtitle"><em class="markup--em markup--h4-em">不要跟我五四三，回點有用的話來</em></h4><figure name="8346" id="8346" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*SCnwW02b4ApvfOgu45S9VA.png" data-width="2616" data-height="1500" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*SCnwW02b4ApvfOgu45S9VA.png"><figcaption class="imageCaption">My girl</figcaption></figure><p name="d076" id="d076" class="graf graf--p graf-after--figure">這篇文章是專門為了繁體中文的 RAG 寫的。</p><p name="a8ad" id="a8ad" class="graf graf--p graf-after--p">你有覺得聊天機器人一直在打高空嗎？<br>你有覺得聊天機器人總是瞎掰胡謅嗎？<br>你有覺得聊天機器人經常答非所問嗎？</p><p name="f5eb" id="f5eb" class="graf graf--p graf-after--p">如果你使用的 prompt 已經很精準了，卻還常有上述感受，那很有可能你實作的 RAG 出了根本上的問題。</p><p name="59fc" id="59fc" class="graf graf--p graf-after--p">要讓 RAG 務實有以下幾個重點：</p><ol class="postList"><li name="ac11" id="ac11" class="graf graf--li graf-after--p">必須是要懂繁體中文的 embedding</li><li name="88d7" id="88d7" class="graf graf--li graf-after--li">要有中文語意的分片器</li><li name="e7a0" id="e7a0" class="graf graf--li graf-after--li">一定要是用繁體中文訓練的 LLM</li><li name="ff56" id="ff56" class="graf graf--li graf-after--li">精準的 prompt 連同 system instruction</li><li name="0009" id="0009" class="graf graf--li graf-after--li">沒做 rerank 的效果都差到不行，但 rerank 要懂繁體中文</li></ol><p name="d295" id="d295" class="graf graf--p graf-after--li">能夠做到上述五點，那麼你就可以得到一個穩重且有參考價值的 RAG。</p><h3 name="345e" id="345e" class="graf graf--h3 graf-after--p">核心概覽</h3><p name="04fd" id="04fd" class="graf graf--p graf-after--h3">以下我會提供一個使用 langchain 實作的繁體中文萬用套餐，最重要的是：本地運行，完全免費。</p><p name="397a" id="397a" class="graf graf--p graf-after--p">既免費又萬用的核心元素是下列：</p><ol class="postList"><li name="33ff" id="33ff" class="graf graf--li graf-after--p">懂中文的 embedding：<a href="https://huggingface.co/BAAI/bge-m3" data-href="https://huggingface.co/BAAI/bge-m3" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">BGE-M3</a></li><li name="afd2" id="afd2" class="graf graf--li graf-after--li">中文語意的分片器：<a href="https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/#splitting-text-from-languages-without-word-boundaries" data-href="https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/#splitting-text-from-languages-without-word-boundaries" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">RecursiveCharacterTextSplitter</a></li><li name="5b14" id="5b14" class="graf graf--li graf-after--li">繁體中文訓練的 LLM：<a href="https://taide.tw/index/download-model" data-href="https://taide.tw/index/download-model" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TAIDE</a></li><li name="e0de" id="e0de" class="graf graf--li graf-after--li">懂繁體中文的 rerank：<a href="https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages" data-href="https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">ms-marco-MultiBERT-L-12</a></li></ol><p name="bacd" id="bacd" class="graf graf--p graf-after--li">上述都是免費的，搭配本地運行的框架：</p><ol class="postList"><li name="f654" id="f654" class="graf graf--li graf-after--p"><a href="https://ollama.com/" data-href="https://ollama.com/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Ollama</a></li><li name="f6d9" id="f6d9" class="graf graf--li graf-after--li"><a href="https://github.com/langchain-ai/langchain" data-href="https://github.com/langchain-ai/langchain" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Langchain</a></li></ol><p name="64e4" id="64e4" class="graf graf--p graf-after--li">就可以實作我們要的務實 RAG。</p><h3 name="c861" id="c861" class="graf graf--h3 graf-after--p">具體實踐</h3><p name="b831" id="b831" class="graf graf--p graf-after--h3">把 Ollama 裝好，有 Python 後就開始吧。</p><p name="1ac3" id="1ac3" class="graf graf--p graf-after--p">以下我會用一個 <a href="https://www.atlassian.com/software/confluence" data-href="https://www.atlassian.com/software/confluence" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Confluence</a> 的爬蟲做示範，但整個腳本可以套用在任何繁體中文的資料源。</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="a5f9" id="a5f9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Langchain 的核心套件</span><br />pip install langchain langchain-community langchain-core langchain-huggingface<br /><span class="hljs-comment"># Confluence 爬蟲需要的套件 (根據資料源可替換)</span><br />pip install atlassian-python-api lxml pytesseract docx2txt<br /><span class="hljs-comment"># Rerank 用</span><br />pip install flashrank<br /><span class="hljs-comment"># 向量存儲</span><br />pip install faiss-cpu</span></pre><p name="c29d" id="c29d" class="graf graf--p graf-after--pre">Python 套件都安裝完成後的第一步，我們要先準備給 RAG 使用的資料源，以本例來說是 Confluence。</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0aef" id="0aef" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain_community.document_loaders <span class="hljs-keyword">import</span> ConfluenceLoader<br /><br />CONFLUENCE_TOKEN = <span class="hljs-string">&#x27;&lt;your token&gt;&#x27;</span><br />USER = <span class="hljs-string">&#x27;&lt;your account&gt;&#x27;</span><br />ROOT_URL = <span class="hljs-string">&#x27;&lt;your domain&gt;&#x27;</span><br />SPACE_KEY = <span class="hljs-string">&#x27;&lt;your space&gt;&#x27;</span><br />PAGE_ID = <span class="hljs-string">&#x27;&lt;your page&gt;&#x27;</span><br />loader = ConfluenceLoader(<br />    url=ROOT_URL, username=USER, api_key=CONFLUENCE_TOKEN<br />)<br />documents = loader.load(space_key=SPACE_KEY,<br />    page_id=PAGE_ID, include_attachments=<span class="hljs-literal">True</span>, limit=<span class="hljs-number">50</span>)</span></pre><p name="bc48" id="bc48" class="graf graf--p graf-after--pre">到此，Confluence 的資料就全部抓下來並存進 <code class="markup--code markup--p-code">documents</code> 了。</p><p name="de40" id="de40" class="graf graf--p graf-after--p">若你要處理的是別的資料源，就直接從這裡往下，重點是所有資料要放進 <code class="markup--code markup--p-code">documents</code>。接著我們要來做分段，許多英文的教學會使用 token 來做分段，但中文與 token 並沒有很好的對應，效果會非常差。因此，我們依然採用標點符號的分段方式，並且讓上下兩片段是有重疊的。</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5e4f" id="5e4f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br /><br />text_splitter = RecursiveCharacterTextSplitter(<br />    chunk_size=<span class="hljs-number">800</span>,<br />    chunk_overlap=<span class="hljs-number">400</span>,<br />    length_function=<span class="hljs-built_in">len</span>,<br />    is_separator_regex=<span class="hljs-literal">False</span>,<br />    separators=[<br />        <span class="hljs-string">&quot;\n\n&quot;</span>,<br />        <span class="hljs-string">&quot;\n&quot;</span>,<br />        <span class="hljs-string">&quot; &quot;</span>,<br />        <span class="hljs-string">&quot;.&quot;</span>,<br />        <span class="hljs-string">&quot;,&quot;</span>,<br />        <span class="hljs-string">&quot;\u200b&quot;</span>,  <span class="hljs-comment"># Zero-width space</span><br />        <span class="hljs-string">&quot;\uff0c&quot;</span>,  <span class="hljs-comment"># Fullwidth comma</span><br />        <span class="hljs-string">&quot;\u3001&quot;</span>,  <span class="hljs-comment"># Ideographic comma</span><br />        <span class="hljs-string">&quot;\uff0e&quot;</span>,  <span class="hljs-comment"># Fullwidth full stop</span><br />        <span class="hljs-string">&quot;\u3002&quot;</span>,  <span class="hljs-comment"># Ideographic full stop</span><br />        <span class="hljs-string">&quot;&quot;</span>,<br />    ],<br />)<br />texts = text_splitter.split_documents(documents)</span></pre><p name="0ec2" id="0ec2" class="graf graf--p graf-after--pre">上面程式碼可以看到，我們對中文的全型逗號和句號做了額外處理，想要加入更多字元也行。</p><p name="c1a5" id="c1a5" class="graf graf--p graf-after--p">另一個關鍵是我們的片段長度是 <code class="markup--code markup--p-code">800</code> 且重疊 <code class="markup--code markup--p-code">400</code> 字，這是我實測下來最理想的長度了，當然也可以根據需求調整。</p><p name="82a1" id="82a1" class="graf graf--p graf-after--p">接著我們準備開始做 embedding，務必記得，embedding 的模型一定要懂繁體中文。現階段，既免費效果又好的當屬 <code class="markup--code markup--p-code">BAAI/bge-m3</code>。</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5769" id="5769" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain_huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br /><span class="hljs-keyword">from</span> langchain_community.vectorstores <span class="hljs-keyword">import</span> FAISS<br /><br />HF_EMBEDDING_MODEL = <span class="hljs-string">&#x27;BAAI/bge-m3&#x27;</span><br />hf_embeddings = HuggingFaceEmbeddings(<br />    model_name=HF_EMBEDDING_MODEL,<br />    model_kwargs={<span class="hljs-string">&#x27;device&#x27;</span>: <span class="hljs-string">&#x27;cpu&#x27;</span>},<br />    encode_kwargs={<span class="hljs-string">&#x27;normalize_embeddings&#x27;</span>: <span class="hljs-literal">False</span>}<br />)<br />vectordb = FAISS.from_documents(texts, hf_embeddings)</span></pre><p name="e311" id="e311" class="graf graf--p graf-after--pre">現在我們已經有一份完整的向量放在記憶體內了，至於是否需要落地，也是根據需求決定。</p><p name="ef81" id="ef81" class="graf graf--p graf-after--p">最後就是聊天對話了，讓我們用 langchain 的內建功能快速搭建起來。</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="68cb" id="68cb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br /><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br /><span class="hljs-keyword">from</span> langchain.retrievers.document_compressors <span class="hljs-keyword">import</span> FlashrankRerank<br /><span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> ContextualCompressionRetriever<br /><span class="hljs-keyword">from</span> langchain_community.llms <span class="hljs-keyword">import</span> Ollama<br /><br />LLM_MODEL = <span class="hljs-string">&#x27;cwchang/llama3-taide-lx-8b-chat-alpha1&#x27;</span><br />RERANK_MODEL = <span class="hljs-string">&#x27;ms-marco-MultiBERT-L-12&#x27;</span><br /><br />llm = Ollama(model=LLM_MODEL)<br />custom_prompt_template = <span class="hljs-string">&quot;&quot;&quot;<br />&lt;your system instruction&gt;<br />{context}<br />Question: {question}<br />Helpful Answer:&quot;&quot;&quot;</span><br />CUSTOMPROMPT = PromptTemplate(<br />    template=custom_prompt_template, input_variables=[<span class="hljs-string">&quot;context&quot;</span>, <span class="hljs-string">&quot;question&quot;</span>]<br />)<br />retriever = vectordb.as_retriever(search_type=<span class="hljs-string">&quot;similarity&quot;</span>,<br />    search_kwargs={<span class="hljs-string">&quot;k&quot;</span>: <span class="hljs-number">100</span>}) <span class="hljs-comment"># K1, Top100 Snippets</span><br />compressor = FlashrankRerank(model=RERANK_MODEL, top_n=<span class="hljs-number">5</span>) <span class="hljs-comment"># K2, Top5 Answers</span><br />compression_retriever = ContextualCompressionRetriever(<br />    base_compressor=compressor, base_retriever=retriever<br />)<br />qa = RetrievalQA.from_chain_type(llm=llm, chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, <br />    retriever=compression_retriever, return_source_documents=<span class="hljs-literal">True</span>)<br /><span class="hljs-comment">## Inject custom prompt </span><br />qa.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT<br />question = <span class="hljs-string">&quot;&lt;your question&gt;&quot;</span><br />answer = qa({<span class="hljs-string">&quot;query&quot;</span>: question})<br /><span class="hljs-built_in">print</span>(answer)</span></pre><p name="6a8e" id="6a8e" class="graf graf--p graf-after--pre">上面這段 code 有幾個重點：</p><ol class="postList"><li name="3491" id="3491" class="graf graf--li graf-after--p">採用 Ollama Hub 內有的 TAIDE 模型 (整合好的比較方便)</li><li name="7def" id="7def" class="graf graf--li graf-after--li">Rerank 模型則是 langchain 整合的 <code class="markup--code markup--li-code">ms-marco-MultiBERT-L-12</code></li><li name="534f" id="534f" class="graf graf--li graf-after--li">先用向量的相似性比對找出前 100 名，接著透過 rerank 找出最佳的 5 個</li><li name="9239" id="9239" class="graf graf--li graf-after--li">雖然 <code class="markup--code markup--li-code">answer</code> 只會有一個答案，但 <code class="markup--code markup--li-code">return_source_documents=True</code> 所以可以看到參考了哪些結果</li></ol><h3 name="f95b" id="f95b" class="graf graf--h3 graf-after--li">結論</h3><p name="88a9" id="88a9" class="graf graf--p graf-after--h3">上面那整段 code 都只需要本地運行，硬碟大概需要 10 GB 左右，但因為不需要 GPU ，所以一般手上的電腦都可以跑。</p><p name="c55e" id="c55e" class="graf graf--p graf-after--p">雖然是以 Confluence 爬蟲作為例子，但相信你們都可以發現，langchain 這個框架其實整合的相當好，大部分的工作都可以簡單幾行 code 就處理掉。即使是需要一定 know-how 的 rerank，也已經有內建的整合可以直接使用。</p><p name="25da" id="25da" class="graf graf--p graf-after--p graf--trailing">至於腳本內沒提到的 prompt 和 system instruction，那個必須根據你的用途和語料來動態調整，所以如果使用這個腳本還碰到不夠精準的回答，那多半是這兩點沒調校好。</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/65fc63647c51"><time class="dt-published" datetime="2024-08-19T01:33:51.687Z">August 19, 2024</time></a>.</p><p><a href="https://medium.com/@lazypro/%E5%81%9A%E4%B8%80%E5%80%8B%E5%8B%99%E5%AF%A6%E7%9A%84rag-pragmatic-rag-65fc63647c51" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>