<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Solving Dogpile Effect</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Solving Dogpile Effect</h1>
</header>
<section data-field="subtitle" class="p-summary">
How to cache in high-volume scenarios
</section>
<section data-field="body" class="e-content">
<section name="e387" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1368" id="1368" class="graf graf--h3 graf--leading graf--title">Solving Dogpile Effect</h3><h4 name="1481" id="1481" class="graf graf--h4 graf-after--h3 graf--subtitle">How to cache in high-volume scenarios</h4><figure name="8bc9" id="8bc9" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*RDbkGUsUcA0pzMjr" data-width="1000" data-height="643" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*RDbkGUsUcA0pzMjr"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@jneumeyer" data-href="https://unsplash.com/@jneumeyer" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Judi Neumeyer</a> on <a href="https://unsplash.com/photos/ECjHeJtRznQ" data-href="https://unsplash.com/photos/ECjHeJtRznQ" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure><p name="f751" id="f751" class="graf graf--p graf-after--figure">We have discussed cache consistency before, and at that time we mentioned it is possible to achieve good consistency if we implement read-aside cache correctly. To further improve consistency, a more complex solution must be used, e.g., write through cache or write behind cache.</p><ul class="postList"><li name="995b" id="995b" class="graf graf--li graf-after--p"><a href="https://medium.com/starbugs/consistency-between-cache-and-database-part-1-f64f4a76720" data-href="https://medium.com/starbugs/consistency-between-cache-and-database-part-1-f64f4a76720" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Consistency between Cache and Database, Part 1</a></li><li name="5790" id="5790" class="graf graf--li graf-after--li"><a href="https://medium.com/starbugs/consistency-between-cache-and-database-part-2-e28fc7f8a7c3" data-href="https://medium.com/starbugs/consistency-between-cache-and-database-part-2-e28fc7f8a7c3" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Consistency between Cache and Database, Part 2</a></li></ul><p name="b890" id="b890" class="graf graf--p graf-after--li">Today, we are going to discuss another common scenario of caching, Dogpile Effect.</p><p name="6c89" id="6c89" class="graf graf--p graf-after--p">Dogpile Effect means when the system is under heavy volume of traffic, whenever a cache invalidates, either by cleanup or timeout, it will have a huge impact.</p><p name="4bd7" id="4bd7" class="graf graf--p graf-after--p">For example, if a cache entry is accessed by 100 requests at the same time, once the entry expires, 100 requests will hit the backend system directly, which is a serious challenge for the backend system.</p><p name="d5de" id="d5de" class="graf graf--p graf-after--p">Therefore, let’s see what approaches are available to deal with Dogpile Effect. There are three common approaches as follows.</p><ol class="postList"><li name="9b04" id="9b04" class="graf graf--li graf-after--p">Warm Up Cache</li><li name="a3b6" id="a3b6" class="graf graf--li graf-after--li">Extend Cache Time</li><li name="7a19" id="7a19" class="graf graf--li graf-after--li">Exclusive Lock</li></ol><p name="adea" id="adea" class="graf graf--p graf-after--li">Each of these three options has its own pros and cons, and in fact, they are already widely used.</p><blockquote name="4263" id="4263" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">Question, do you know which one is the </em><code class="markup--code markup--blockquote-code"><em class="markup--em markup--blockquote-em">race_condition_ttl</em></code><em class="markup--em markup--blockquote-em"> of the cache store in </em>Ruby on Rails<em class="markup--em markup--blockquote-em">?</em></blockquote><h3 name="2382" id="2382" class="graf graf--h3 graf-after--blockquote">Warm Up Cache</h3><p name="70e4" id="70e4" class="graf graf--p graf-after--h3">Before we talk about this approach, let’s review the read path of the read-aside cache.</p><ol class="postList"><li name="b5ae" id="b5ae" class="graf graf--li graf-after--p">First, all read requests are read from the cache.</li><li name="d368" id="d368" class="graf graf--li graf-after--li">If the cache fails to read.</li><li name="79e7" id="79e7" class="graf graf--li graf-after--li">Then read from the database, and write back to the cache.</li></ol><p name="b7ff" id="b7ff" class="graf graf--p graf-after--li">The problem occurs when the cache is missed.</p><p name="cb26" id="cb26" class="graf graf--p graf-after--p">When there are lots of read requests under a high volume system, it will result in lots of database accesses. If we can keep the cache active anyway, can’t we solve Dogpile Effect?</p><p name="a19b" id="a19b" class="graf graf--p graf-after--p">Thus, the approach is to replace the cache TTL with a background thread that updates all caches periodically, e.g., if the cache TTL is 5 minutes, then update all caches every 5 minutes, so that cache invalidation is no longer encountered.</p><p name="b838" id="b838" class="graf graf--p graf-after--p">But such an approach has its drawbacks.</p><ol class="postList"><li name="a01a" id="a01a" class="graf graf--li graf-after--p">It is very space inefficient if applied to all cache entries.</li><li name="0ce3" id="0ce3" class="graf graf--li graf-after--li">In some corner cases, caching still invalidates, such as an eviction triggered by cache itself.</li></ol><p name="ed50" id="ed50" class="graf graf--p graf-after--li">Although this is a simple and brutal approach, I have to say it works very well in some situations. If we have a critical cache that has to deal with a lot of traffic and the cost of updating is high, then keeping that cache fresh is the most effective way.</p><p name="27ee" id="27ee" class="graf graf--p graf-after--p">As long as we avoid updating all cache entries, then it won’t take up a lot of space and will be less likely to trigger corner cases.</p><h3 name="f7fd" id="f7fd" class="graf graf--h3 graf-after--p">Extend Cache Time</h3><p name="3c48" id="3c48" class="graf graf--p graf-after--h3">Warm up cache can work well on specific critical cache, but if no critical cache is defined at all, then the benefits of warm up cannot be applied effectively.</p><p name="5113" id="5113" class="graf graf--p graf-after--p">Therefore, the second approach is suitable for general purposes.</p><p name="c54f" id="c54f" class="graf graf--p graf-after--p">When reading a cache, if a cache timeout is found, then extend the cache time slightly and start updating the cache. If there is a concurrent read request, the later read request will read the cache with the extended time to avoid accessing the backend database simultaneously.</p><p name="5fba" id="5fba" class="graf graf--p graf-after--p">Assuming that the cache TTL is 5 minutes, we set each cache to have a one-minute extension time, as shown in the Gantt chart below.</p><figure name="b430" id="b430" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*T3PfOREet6DOBKdNoI7Ukg.png" data-width="1096" data-height="244" src="https://cdn-images-1.medium.com/max/800/1*T3PfOREet6DOBKdNoI7Ukg.png"></figure><p name="539e" id="539e" class="graf graf--p graf-after--figure">In the time interval 0–5, reading the cache will get the original value. If someone reads during the time interval 5–6, although the cache expires, the cache will be extended, so the original value will still be available. But at the same time, the first person who reads in the interval 5–6 must be responsible for updating the cache, so after the time point 6, the cache will be updated to the new value.</p><p name="8f31" id="8f31" class="graf graf--p graf-after--p">Let’s use a sequential diagram to represent the two concurrent request scenarios.</p><figure name="9974" id="9974" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Ct1Gead4Mz060ScHgw-c3g.png" data-width="670" data-height="628" src="https://cdn-images-1.medium.com/max/800/1*Ct1Gead4Mz060ScHgw-c3g.png"></figure><p name="a6c4" id="a6c4" class="graf graf--p graf-after--figure">Suppose the cache has already timed out. When <code class="markup--code markup--p-code">A</code> fetches, it finds that the cache has timed out, so it first writes the original value back to the cache and performs the regular read operation to fetch the value from the database, and finally writes the new value back to the cache.</p><p name="f848" id="f848" class="graf graf--p graf-after--p">However, <code class="markup--code markup--p-code">B</code> finds that the cache has not expired when it reads, because the cache is extended, so it can get the original value.</p><p name="29d8" id="29d8" class="graf graf--p graf-after--p">With this approach, only one of the <code class="markup--code markup--p-code">N</code> concurrent requests needs to access the backend, and the rest of the <code class="markup--code markup--p-code">N - 1</code> requests can still fetch the value from the cache.</p><div name="4847" id="4847" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/rails/rails/blob/e5f3d692030b41ed5ac671fedc9cc268b13ebd40/activesupport/lib/active_support/cache.rb#L849" data-href="https://github.com/rails/rails/blob/e5f3d692030b41ed5ac671fedc9cc268b13ebd40/activesupport/lib/active_support/cache.rb#L849" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/rails/rails/blob/e5f3d692030b41ed5ac671fedc9cc268b13ebd40/activesupport/lib/active_support/cache.rb#L849"><strong class="markup--strong markup--mixtapeEmbed-strong">rails/cache.rb at e5f3d692030b41ed5ac671fedc9cc268b13ebd40 · rails/rails</strong><br><em class="markup--em markup--mixtapeEmbed-em">This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below…</em>github.com</a><a href="https://github.com/rails/rails/blob/e5f3d692030b41ed5ac671fedc9cc268b13ebd40/activesupport/lib/active_support/cache.rb#L849" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e226009609c42db2a358d0ef3d86f1e8" data-thumbnail-img-id="0*adbK-7chpKukurHk" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*adbK-7chpKukurHk);"></a></div><p name="7c84" id="7c84" class="graf graf--p graf-after--mixtapeEmbed">Actually, <em class="markup--em markup--p-em">Ruby on Rails</em>’s <code class="markup--code markup--p-code">race_condition_ttl</code> is this implementation. Lines 855 and 856 in the link above are the operations that extend the cache time.</p><p name="4166" id="4166" class="graf graf--p graf-after--p">This approach looks like an effective way to handle high volume traffic, with only one request for access to the backend, right?</p><p name="af14" id="af14" class="graf graf--p graf-after--p">The answer is, no, not really.</p><p name="9f8b" id="9f8b" class="graf graf--p graf-after--p">This is obviously useless when facing a high-concurrency scenario. Let’s continue to describe the problem in a sequential diagram.</p><figure name="df3f" id="df3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5NfKSOns_sXAuXcgXr_Umw.png" data-width="730" data-height="874" src="https://cdn-images-1.medium.com/max/800/1*5NfKSOns_sXAuXcgXr_Umw.png"></figure><p name="a3a7" id="a3a7" class="graf graf--p graf-after--figure">The same <code class="markup--code markup--p-code">A</code> and <code class="markup--code markup--p-code">B</code> as earlier, but this time <code class="markup--code markup--p-code">A</code> and <code class="markup--code markup--p-code">B</code> occurred very close to each other. As you can see from the sequential diagram, <code class="markup--code markup--p-code">B</code> has already happened when <code class="markup--code markup--p-code">A</code> tries to write the original value back to the cache, so <code class="markup--code markup--p-code">B</code> also feels he is the first one.</p><p name="7f19" id="7f19" class="graf graf--p graf-after--p">When <code class="markup--code markup--p-code">N</code> requests arrive completely at the same time, then such an implementation still cannot solve Dogpile Effect.</p><h3 name="d9fb" id="d9fb" class="graf graf--h3 graf-after--p">Exclusive lock</h3><p name="9361" id="9361" class="graf graf--p graf-after--h3">Extending the cache time seems to have solved most of the problems, but it’s still not good enough in a high-concurrency system. Therefore, we need a way to serialize the high-concurrency scenario. Previously, we introduced the concept of the exclusive lock.</p><div name="cd7d" id="cd7d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://betterprogramming.pub/redis-as-a-lock-are-you-sure-a870c9f22ad8" data-href="https://betterprogramming.pub/redis-as-a-lock-are-you-sure-a870c9f22ad8" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://betterprogramming.pub/redis-as-a-lock-are-you-sure-a870c9f22ad8"><strong class="markup--strong markup--mixtapeEmbed-strong">Redis as a Lock! Are You Sure?</strong><br><em class="markup--em markup--mixtapeEmbed-em">Is the lock you are talking about an exclusive lock or a barrier?</em>betterprogramming.pub</a><a href="https://betterprogramming.pub/redis-as-a-lock-are-you-sure-a870c9f22ad8" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c65a848d8e3a5796290bd7053175b391" data-thumbnail-img-id="0*ruuHg6RyBfet0s48" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ruuHg6RyBfet0s48);"></a></div><p name="b7d3" id="b7d3" class="graf graf--p graf-after--mixtapeEmbed">In this approach, we are trying to avoid multiple concurrent requests to go through the cache by exclusive lock.</p><p name="5407" id="5407" class="graf graf--p graf-after--p">First, an exclusive lock must be acquired before the cache can be updated. Only those who can acquire the lock are eligible to update the cache, i. e. access the database, while the rest of the people who do not acquire the lock must wait for one of the following two conditions.</p><ol class="postList"><li name="2c30" id="2c30" class="graf graf--li graf-after--p">whether the cache has been updated</li><li name="a9e9" id="a9e9" class="graf graf--li graf-after--li">whether the lock can be acquired</li></ol><p name="f0c7" id="f0c7" class="graf graf--p graf-after--li">The wait-and-acquire lock must also verify whether the cache has been updated in order to further avoid duplicate database accesses.</p><p name="8f0e" id="8f0e" class="graf graf--p graf-after--p">Of course, there is an obvious drawback to this approach. Making the concurrent process serializable would significantly reduce concurrency. In addition, the waiting is an extra overhead that not only consumes resources but also affects performance.</p><p name="758f" id="758f" class="graf graf--p graf-after--p">It’s worth mentioning that we often say Redis is not reliable, so in this scenario, is it necessary to use Redlock to further improve reliability?</p><p name="c3f8" id="c3f8" class="graf graf--p graf-after--p">It depends.</p><p name="0477" id="0477" class="graf graf--p graf-after--p">In general, no, even if Redis is not reliable enough to be used exactly once, the rare occurrence of a leak is not a problem in this scenario. This is not a scenario that requires strong consistency, but at best a few accesses to the database.</p><h3 name="e9f5" id="e9f5" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="3637" id="3637" class="graf graf--p graf-after--h3">In order to address a common problem with caching, Dogpile Effect, we have reviewed three common approaches.</p><p name="4981" id="4981" class="graf graf--p graf-after--p">The scenario where the warm up cache is applicable is the “critical cache”. Once we can narrow down the scope of the cache, then keeping it fresh is the most intuitive and easy way for us.</p><p name="9f73" id="9f73" class="graf graf--p graf-after--p">Extending the cache time is a general-purpose approach that can effectively tackle a variety of high-volume scenarios. By providing a time buffer, the cache is allowed to serve for a long time until the cache is updated by “someone”.</p><p name="fd14" id="fd14" class="graf graf--p graf-after--p">Exclusive lock is a high-concurrency specialization approach. In order to avoid concurrent requests from hitting the backend system, concurrent requests are turned into sequential requests through a serializable mechanism.</p><p name="b89a" id="b89a" class="graf graf--p graf-after--p">Each of these three approaches has its own advantages and benefits, but in fact, extend cache time and exclusive lock can be combined into a total solution. I will explain the implementation details of this in code in the next article.</p><p name="4e23" id="4e23" class="graf graf--p graf-after--p graf--trailing">Let’s call it a day.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/9d869174d302"><time class="dt-published" datetime="2022-11-14T01:59:51.278Z">November 14, 2022</time></a>.</p><p><a href="https://medium.com/@lazypro/solving-dogpile-effect-9d869174d302" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>