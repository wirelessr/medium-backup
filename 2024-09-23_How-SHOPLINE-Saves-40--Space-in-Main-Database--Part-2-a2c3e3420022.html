<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How SHOPLINE Saves 40% Space in Main Database, Part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How SHOPLINE Saves 40% Space in Main Database, Part 2</h1>
</header>
<section data-field="subtitle" class="p-summary">
An in-depth guide to practical data tiering and advanced technical solutions
</section>
<section data-field="body" class="e-content">
<section name="328b" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b0f8" id="b0f8" class="graf graf--h3 graf--leading graf--title">How SHOPLINE Saves 40% Space in Main Database, Part 2</h3><h4 name="9293" id="9293" class="graf graf--h4 graf-after--h3 graf--subtitle">An in-depth guide to practical data tiering and advanced technical solutions</h4><figure name="b1f6" id="b1f6" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*O2IDsBM28q-_1tzKwU12gw.png" data-width="1962" data-height="1216" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*O2IDsBM28q-_1tzKwU12gw.png"><figcaption class="imageCaption">My girl</figcaption></figure><blockquote name="023a" id="023a" class="graf graf--blockquote graf-after--figure">Not a member? You can still check out this article through <a href="https://lazypro.medium.com/how-shopline-saves-40-space-in-main-database-part-2-a2c3e3420022?sk=3dffe1ae158de4e4c2ec2bfde2be45e9" data-href="https://lazypro.medium.com/how-shopline-saves-40-space-in-main-database-part-2-a2c3e3420022?sk=3dffe1ae158de4e4c2ec2bfde2be45e9" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">here</a>.</blockquote><ul class="postList"><li name="b6f6" id="b6f6" class="graf graf--li graf-after--blockquote">Part 1: <a href="https://medium.com/stackademic/how-shopline-saves-40-space-in-main-database-9504a1eb2472" data-href="https://medium.com/stackademic/how-shopline-saves-40-space-in-main-database-9504a1eb2472" class="markup--anchor markup--li-anchor" target="_blank">Data tiering methodology</a></li><li name="f5df" id="f5df" class="graf graf--li graf-after--li graf--trailing">Part 2 (this article): Practice detail</li></ul></div></div></section><section name="d895" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><blockquote name="1782" id="1782" class="graf graf--blockquote graf--leading"><em class="markup--em markup--blockquote-em">If you haven’t read Part 1 yet, we suggest you start there. In Part 1 we went through the complete steps of how to start data tiering from the business side.</em></blockquote><p name="7584" id="7584" class="graf graf--p graf-after--blockquote">Let’s quickly review the process from the previous article.</p><ol class="postList"><li name="8dee" id="8dee" class="graf graf--li graf-after--p">Identify the target that needs to be tiered.</li><li name="99f2" id="99f2" class="graf graf--li graf-after--li">Set the time partitioning for data tiering to occur.</li><li name="f186" id="f186" class="graf graf--li graf-after--li">Define the user scenarios for the cold data.</li><li name="fef6" id="fef6" class="graf graf--li graf-after--li">Verify our hypothesis.</li><li name="5ee5" id="5ee5" class="graf graf--li graf-after--li">Make the right technical selection.</li><li name="c5da" id="c5da" class="graf graf--li graf-after--li">Begin implementation.</li></ol><p name="f2a8" id="f2a8" class="graf graf--p graf-after--li">When SHOPLINE decides to perform data tiering, based on the above process, we need to know which target to start with first.</p><p name="4eef" id="4eef" class="graf graf--p graf-after--p">After actually counting all the tables, we found that the order related data takes up more than 80% of the main database capacity, and among them, the order and its metadata are the most significant.</p><p name="7c0f" id="7c0f" class="graf graf--p graf-after--p">Therefore, we finally selected order and order metadata as the first target for data tiering, they are <code class="markup--code markup--p-code">order</code>, <code class="markup--code markup--p-code">order_item</code>, <code class="markup--code markup--p-code">order_payment</code> and <code class="markup--code markup--p-code">order_delivery</code>.</p><p name="e789" id="e789" class="graf graf--p graf-after--p">With specific targets in mind, we then need to figure out what kind of time partitioning we want to use in order to minimize the impact on users.</p><p name="5be5" id="5be5" class="graf graf--p graf-after--p">We collected all the REST API calls and their input parameters over a period of time and counted the time interval in which these input parameters were used. From this information, we observed that when an order is older than two years, the frequency of usage decreases significantly, so two years becomes our hypothesis for cold data.</p><p name="bcc3" id="bcc3" class="graf graf--p graf-after--p">At the same time, the data tiering we want is not to make the data disappear completely, but rather to archive the cold data in a place where it can be used for basic querying.</p><p name="d055" id="d055" class="graf graf--p graf-after--p">Although we no longer support complex query criteria and fast response times, we still want users to be able to get the information they want when they really need it, such as information about a particular order or a certain period of time.</p><figure name="12f7" id="12f7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*HcsyO0YNwDD7VKxR.png" data-width="1306" data-height="648" src="https://cdn-images-1.medium.com/max/800/0*HcsyO0YNwDD7VKxR.png"></figure><p name="fffe" id="fffe" class="graf graf--p graf-after--figure">This is exactly strategy 4: data warehousing.</p><p name="29b7" id="29b7" class="graf graf--p graf-after--p">In addition, there are many metadata in the order, and even though we decided to archive <code class="markup--code markup--p-code">order_item</code>, <code class="markup--code markup--p-code">order_payment</code>, and <code class="markup--code markup--p-code">order_delivery</code>, we still want to get the rest of the information in a single query in the data warehouse. Then, we still need to find a way to validate which metadata we want to archive.</p><p name="36b4" id="36b4" class="graf graf--p graf-after--p">At first, we made a few changes to the existing UI to observe user behavior. We want to clarify the following issues.</p><ol class="postList"><li name="93af" id="93af" class="graf graf--li graf-after--p">How often users view old data.</li><li name="1089" id="1089" class="graf graf--li graf-after--li">What kind of old data the user specifically needs.</li></ol><p name="cfda" id="cfda" class="graf graf--p graf-after--li">Our approach is simple: we have added a two-year restriction to the existing time-selection component so that users cannot select a time period longer than two years. Nevertheless, we will not block the user if he queries directly with the order number.</p><p name="d5e6" id="d5e6" class="graf graf--p graf-after--p">However, we will mask the order details section so that order details older than two years are collapsed based on metadata. When the user clicks on the exact section, we can track which metadata the user really cares about.</p><p name="9220" id="9220" class="graf graf--p graf-after--p">Here’s a little trick.</p><p name="27c7" id="27c7" class="graf graf--p graf-after--p">If we collapse all the details, the user’s first reaction will be to click on all of them. Therefore, we opened the basic information of the order, as well as the payment &amp; logistics, and collapsed only those metadata sections that we wanted to validate.</p><p name="3cba" id="3cba" class="graf graf--p graf-after--p">In the end, we verified both the time hypothesis and the user’s need for metadata. Then, we can begin to formally enter the technical selection phase.</p><h3 name="65fd" id="65fd" class="graf graf--h3 graf-after--p">Advanced technical selection</h3><p name="abb6" id="abb6" class="graf graf--p graf-after--h3">Although we chose data warehousing for cold data storage based on the user scenario, there are still many data warehousing options to choose from.</p><p name="59c6" id="59c6" class="graf graf--p graf-after--p">One of the most important considerations is the existing technical stack.</p><p name="3050" id="3050" class="graf graf--p graf-after--p">As mentioned in the previous article, if the existing database naturally supports data tiering, then there will not be so many problems. Also, depending on the existing data pipeline in the organization, there may be viable options.</p><p name="7833" id="7833" class="graf graf--p graf-after--p">Let’s take a quick look at the SHOPLINE technical stack.</p><ul class="postList"><li name="855c" id="855c" class="graf graf--li graf-after--p">The (former) main database: MongoDB Atlas</li><li name="618d" id="618d" class="graf graf--li graf-after--li">Main database for data mart: PingCAP TiDB</li><li name="3691" id="3691" class="graf graf--li graf-after--li">Main Warehouse for Data Pipeline: GCP BigQuery</li></ul><p name="9b85" id="9b85" class="graf graf--p graf-after--li">There are three technology options that can be born from these stacks.</p><h3 name="123d" id="123d" class="graf graf--h3 graf-after--p"><a href="https://www.mongodb.com/products/platform/atlas-data-federation" data-href="https://www.mongodb.com/products/platform/atlas-data-federation" class="markup--anchor markup--h3-anchor" rel="noopener" target="_blank">Atlas Data Federation</a></h3><p name="53d5" id="53d5" class="graf graf--p graf-after--h3">Data federation is a real-time streaming pipeline developed by Atlas that delivers MongoDB data to the object storage system in a near real-time way.</p><p name="8903" id="8903" class="graf graf--p graf-after--p">It also builds an API on the object store to provide querying via MQL.</p><figure name="113c" id="113c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*VYGtRBhjJr9SEsQ4ATmFIQ.png" data-width="658" data-height="492" src="https://cdn-images-1.medium.com/max/800/1*VYGtRBhjJr9SEsQ4ATmFIQ.png"></figure><p name="bb53" id="bb53" class="graf graf--p graf-after--figure">I have to say, this is an excellent solution. Unlike regular data warehouses that offer SQL capabilities, this Atlas self developed data warehouse offers MQL querying capabilities. For an application that is already using MongoDB and MQL, it’s almost a seamless transition.</p><p name="289d" id="289d" class="graf graf--p graf-after--p">Furthermore, the data written to this data warehouse can be generated based on specific aggregation statements, making it quicker to satisfy a variety of scenarios.</p><p name="8733" id="8733" class="graf graf--p graf-after--p">Nevertheless, Atlas Federation is a technology that can’t be duplicated — after all, it’s Atlas’ own data warehouse. In other words, once we’ve chosen this solution, we’re vendor lock-in.</p><h3 name="decb" id="decb" class="graf graf--h3 graf-after--p"><a href="https://www.pingcap.com/tidb-serverless/" data-href="https://www.pingcap.com/tidb-serverless/" class="markup--anchor markup--h3-anchor" rel="noopener" target="_blank">PingCAP TiDB Serverless</a></h3><p name="be3e" id="be3e" class="graf graf--p graf-after--h3">TiDB Serverless is a Compute-Storage Separation architecture, similar to regular data warehouse. It builds a one-tier query engine on top of the object storage system and provides SQL querying capabilities.</p><figure name="e2fa" id="e2fa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*J_1vJ4hWjpo-KMO3WW2AkA.png" data-width="524" data-height="656" src="https://cdn-images-1.medium.com/max/800/1*J_1vJ4hWjpo-KMO3WW2AkA.png"></figure><p name="906a" id="906a" class="graf graf--p graf-after--figure">When there are no queries at all, the query engine can be scaled to zero to save costs. In other words, when not in use, TiDB Serverless is only a storage cost. This use case is ideal for cold data storage after data tiering.</p><p name="d55d" id="d55d" class="graf graf--p graf-after--p">For an OLTP database, this would be a brand new invention, but for regular OLAP data warehouse, this is not a big advantage.</p><p name="f0e3" id="f0e3" class="graf graf--p graf-after--p">For us, if we use this solution, on the one hand, it is another vendor lock-in and on the other hand, we need to rewrite the application program significantly to support SQL query.</p><p name="e6ea" id="e6ea" class="graf graf--p graf-after--p">If we need to rewrite the application to support SQL, why not use a regular data warehouse such as BigQuery, which we are already using?</p><h3 name="f288" id="f288" class="graf graf--h3 graf-after--p">GCP BigQuery</h3><p name="4244" id="4244" class="graf graf--p graf-after--h3">I have described our data pipeline in <a href="https://medium.com/stackademic/tidb-kappa-accelerating-api-responses-with-smart-architecture-5fe9a28f0f83" data-href="https://medium.com/stackademic/tidb-kappa-accelerating-api-responses-with-smart-architecture-5fe9a28f0f83" class="markup--anchor markup--p-anchor" target="_blank">my previous article</a>.</p><figure name="e262" id="e262" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MwieNFNPy6SCpqT1K8cepA.png" data-width="822" data-height="304" src="https://cdn-images-1.medium.com/max/800/1*MwieNFNPy6SCpqT1K8cepA.png"></figure><p name="8662" id="8662" class="graf graf--p graf-after--figure">One of the challenges of using BigQuery as a data warehouse for cold data is that BigQuery is not low cost to query. If directly expose BigQuery to the application, then any misuse could be costly, so for the application, we will send the well organized data to the data mart.</p><p name="1b5c" id="1b5c" class="graf graf--p graf-after--p">If we put all the cold data from all the orders into the data mart, we are creating another object that requires data tiering. But if we don’t use the data mart, then BigQuery would have to be exposed to application queries, which is against our data governance policy.</p><p name="4e6a" id="4e6a" class="graf graf--p graf-after--p">Second, BigQuery is also a vendor lock-in that we want to get rid of, so we don’t want to add more use cases to BigQuery.</p><h3 name="75f4" id="75f4" class="graf graf--h3 graf-after--p">Self-built Data Warehouse</h3><p name="c295" id="c295" class="graf graf--p graf-after--h3">In fact, we have been investigating the option of building our own data warehouse.</p><p name="7fda" id="7fda" class="graf graf--p graf-after--p">The so-called data warehouse is actually a nearly unlimited and low-cost storage space coupled with a scalable computing engine. The lowest entry point is to store Iceberg in AWS S3 and provide query capabilities through Trino.</p><p name="c246" id="c246" class="graf graf--p graf-after--p">To investigate the feasibility of Iceberg, I’ve made many attempts to produce several interesting open source playgrounds.</p><ul class="postList"><li name="a89e" id="a89e" class="graf graf--li graf-after--p"><a href="https://medium.com/@lazypro/getting-started-with-flink-sql-apache-iceberg-and-dynamodb-catalog-71b96817e3c3" data-href="https://medium.com/@lazypro/getting-started-with-flink-sql-apache-iceberg-and-dynamodb-catalog-71b96817e3c3" class="markup--anchor markup--li-anchor" target="_blank">Integrating Flink with Iceberg</a></li><li name="e298" id="e298" class="graf graf--li graf-after--li"><a href="https://medium.com/@lazypro/trino-iceberg-made-easy-a-ready-to-use-playground-7f4a1f7713ca" data-href="https://medium.com/@lazypro/trino-iceberg-made-easy-a-ready-to-use-playground-7f4a1f7713ca" class="markup--anchor markup--li-anchor" target="_blank">Integrating Trino with Iceberg</a></li></ul><p name="ebf5" id="ebf5" class="graf graf--p graf-after--li">Since it is a practical and feasible approach and is related to our long-term architectural transformation, it will be our answer.</p><h3 name="3987" id="3987" class="graf graf--h3 graf-after--p">Leverage the existing data pipeline (Fail)</h3><p name="6181" id="6181" class="graf graf--p graf-after--h3">At the beginning, our idea was pretty simple, that is, to leverage the existing data pipeline as much as possible, just put the data into another storage in another format.</p><p name="849a" id="849a" class="graf graf--p graf-after--p">So we defined the following architecture.</p><figure name="d7ec" id="d7ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*TbB3VDeLGtyotQXSoWpegA.png" data-width="958" data-height="304" src="https://cdn-images-1.medium.com/max/800/1*TbB3VDeLGtyotQXSoWpegA.png"></figure><p name="e174" id="e174" class="graf graf--p graf-after--figure">As shown in the diagram, most of our components are ready to go, we just need to establish the ability to read and write to Iceberg, which means we can use a real-time data pipeline to accomplish the conversion from hot data to cold data.</p><p name="de81" id="de81" class="graf graf--p graf-after--p">When the life cycle of the data reaches its end, we can delete it in real time, because there is a data pipeline behind it that keeps synchronizing all the data.</p><p name="2e02" id="2e02" class="graf graf--p graf-after--p">The data we want to store is actually what the documents look like in MongoDB, just in Iceberg format in S3.</p><p name="2c16" id="2c16" class="graf graf--p graf-after--p">The following ER model is our data model.</p><figure name="9d71" id="9d71" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IdRk3xFDJUyqASuU9UgF4g.png" data-width="958" data-height="798" src="https://cdn-images-1.medium.com/max/800/1*IdRk3xFDJUyqASuU9UgF4g.png"></figure><p name="c757" id="c757" class="graf graf--p graf-after--figure">The <code class="markup--code markup--p-code">orderID</code> of the <code class="markup--code markup--p-code">order</code> table is a foreign key to the other metadata tables, so only one SQL command is needed to list the orders in a time period.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="9469" id="9469" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">SELECT</span><br />    o.orderID,<br />    o.tenantID,<br />    oi.orderItemID,<br />    ...<br /><span class="hljs-keyword">FROM</span><br />    <span class="hljs-keyword">order</span> o<br /><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span><br />    order_item oi <span class="hljs-keyword">ON</span> o.orderID <span class="hljs-operator">=</span> oi.orderID<br /><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span><br />    order_payment op <span class="hljs-keyword">ON</span> o.orderID <span class="hljs-operator">=</span> op.orderID<br /><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span><br />    order_delivery od <span class="hljs-keyword">ON</span> o.orderID <span class="hljs-operator">=</span> od.orderID<br /><span class="hljs-keyword">WHERE</span><br />    o.orderDate <span class="hljs-keyword">BETWEEN</span> <span class="hljs-string">&#x27;2024-01-01&#x27;</span> <span class="hljs-keyword">AND</span> <span class="hljs-string">&#x27;2024-01-31&#x27;</span><br /><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span><br />    o.orderID;</span></pre><p name="ee9e" id="ee9e" class="graf graf--p graf-after--pre">However, we soon realized a problem. It was difficult to choose a common partition key.</p><p name="baab" id="baab" class="graf graf--p graf-after--p">The reason why a common partition key is desirable is because we don’t want to couple the data pipeline with the business logic, so each table should be treated in the same way.</p><p name="b514" id="b514" class="graf graf--p graf-after--p">Wouldn’t it be better to use <code class="markup--code markup--p-code">orderID</code>? It&#39;s generic and makes business logic.</p><p name="9b03" id="9b03" class="graf graf--p graf-after--p">No, it’s not.</p><p name="0151" id="0151" class="graf graf--p graf-after--p">While <code class="markup--code markup--p-code">orderID</code> may seem like the most appropriate option, in Iceberg&#39;s format, the partition key is the core of file granularity, so using <code class="markup--code markup--p-code">orderID</code> as the partition key will result in an endless number of small files scattered across the object store.</p><p name="6c4d" id="6c4d" class="graf graf--p graf-after--p">This will not only cause inefficiency in querying but also increase the querying cost significantly.</p><p name="c22d" id="c22d" class="graf graf--p graf-after--p">How about using <code class="markup--code markup--p-code">tenantID</code> as the partition key? This is generic enough and makes business logic as well. But as we can see from the ER model above, those metadata tables don&#39;t have <code class="markup--code markup--p-code">tenantID</code> at all.</p><p name="77a3" id="77a3" class="graf graf--p graf-after--p">In that case, we’ll settle for <code class="markup--code markup--p-code">createdAt</code>, which every table has. This may seem like a viable option, but it&#39;s not actually the case. Because the creation date of deliveries, payments, and even order items may not always be the same as the order, this makes it extremely challenging to write a JOIN query using a partition key.</p><p name="0697" id="0697" class="graf graf--p graf-after--p">It seems hard to leverage the existing data pipeline and we have to rethink it.</p><h3 name="8c1b" id="8c1b" class="graf graf--h3 graf-after--p">Creating a cold data wide table</h3><p name="b816" id="b816" class="graf graf--p graf-after--h3">Since the original generic ER model is difficult to model in Iceberg, we need to change our implementation.</p><p name="bb9b" id="bb9b" class="graf graf--p graf-after--p">First of all, we survey all the scenarios where cold data is used, and we realized what we need is to use the <code class="markup--code markup--p-code">order</code> table as the aggregation root, and take out all the related metadata at once. Then it&#39;s easy, we just need to create a wide table.</p><p name="d9a5" id="d9a5" class="graf graf--p graf-after--p">But who is going to create the wide table?</p><p name="f67e" id="f67e" class="graf graf--p graf-after--p">Not the data pipeline definitely. Because <code class="markup--code markup--p-code">order</code> as the aggregation root has domain logic, if we let the data pipeline build the wide table, then it means the domain logic has to be copied into the data pipeline as well, which greatly increases the coupling of business logic.</p><p name="3c94" id="3c94" class="graf graf--p graf-after--p">Finally, we design a new cold down process.</p><figure name="6a3c" id="6a3c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2Ko9AbzuIf9PJpe3msxXhA.png" data-width="958" data-height="388" src="https://cdn-images-1.medium.com/max/800/1*2Ko9AbzuIf9PJpe3msxXhA.png"></figure><p name="3dac" id="3dac" class="graf graf--p graf-after--figure">We first set up a tiny MongoDB as a relay to store the wide tables, so that the data pipeline only needs to receive the results of the aggregation. In conclusion, the data pipeline has no business logic at all, and it’s up to the application to decide what data it wants to collect.</p><p name="54a4" id="54a4" class="graf graf--p graf-after--p">The application can delete the processed hot data after the aggregation, and any scenario that requires cold data can be obtained directly through Trino.</p><h3 name="b939" id="b939" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="6130" id="6130" class="graf graf--p graf-after--h3">In fact, the adoption of self-built data lakehouse for data tiering architecture is our first attempt to transform our data infrastructure.</p><p name="6940" id="6940" class="graf graf--p graf-after--p">Architecture transformation is a huge task, through each small task, we can gradually divide and implement the whole big task. In the case of this data tiering, it plays the role of a pioneer, allowing our organization to become familiar with the use of the data lakehouse.</p><p name="ddf8" id="ddf8" class="graf graf--p graf-after--p">When we have control over the data lakehouse we have built, we have the capital to move on to the next step.</p><p name="2170" id="2170" class="graf graf--p graf-after--p graf--trailing">As a result, not only did we reduce the space in our main database by 40%, but we also learned how to implement and maintain a data lakehouse. With this experience, we are now able to face more complex challenges.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/a2c3e3420022"><time class="dt-published" datetime="2024-09-23T01:49:09.598Z">September 23, 2024</time></a>.</p><p><a href="https://medium.com/@lazypro/how-shopline-saves-40-space-in-main-database-part-2-a2c3e3420022" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>