<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Analyze How to Scale Web Services From Various Perspectives</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Analyze How to Scale Web Services From Various Perspectives</h1>
</header>
<section data-field="subtitle" class="p-summary">
Scaling web services
</section>
<section data-field="body" class="e-content">
<section name="cfff" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8a79" id="8a79" class="graf graf--h3 graf--leading graf--title">Analyzing How to Scale Web Services From Various Perspectives</h3><h4 name="14d6" id="14d6" class="graf graf--h4 graf-after--h3 graf--subtitle">Scaling web services</h4><figure name="6563" id="6563" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*EyDbDI5Dq0SYCUj3.png" data-width="774" data-height="1161" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*EyDbDI5Dq0SYCUj3.png"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@joshuaearle" data-href="https://unsplash.com/@joshuaearle" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Joshua Earle</a> on <a href="https://unsplash.com/photos/oz1GKlRWab4" data-href="https://unsplash.com/photos/oz1GKlRWab4" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure><p name="be48" id="be48" class="graf graf--p graf-after--figure">This article is an internal training for my team to explain to newcomers what are the practices and what are the aspects to consider when scaling a web service.</p><p name="803c" id="803c" class="graf graf--p graf-after--p">The scaling mentioned in this article is divided into several different levels.</p><ol class="postList"><li name="e825" id="e825" class="graf graf--li graf-after--p">Read loading</li><li name="3c45" id="3c45" class="graf graf--li graf-after--li">Write loading</li><li name="8fec" id="8fec" class="graf graf--li graf-after--li">Data volume size</li><li name="c1fa" id="c1fa" class="graf graf--li graf-after--li">Task loading</li><li name="2fdb" id="2fdb" class="graf graf--li graf-after--li">User distribution</li></ol><p name="d260" id="d260" class="graf graf--p graf-after--li">In addition, there is another level of scaling that is not explained in detail in this article, feature scaling, but I will describe the concept of feature scaling a little bit at the end of the article.</p><p name="2147" id="2147" class="graf graf--p graf-after--p">Next, let’s evolve our system step by step.</p><h3 name="1df7" id="1df7" class="graf graf--h3 graf-after--p">Bare Metal</h3><p name="ab3d" id="ab3d" class="graf graf--p graf-after--h3">All products start with a single machine. For proof of concept, we take the simplest approach and put everything on the same box, perhaps the laptop at hand. Whether it’s a physical machine, a virtual machine, or a container, a single machine with everything you need on it is as follows.</p><figure name="02bd" id="02bd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mJhaHQvfyy8c9Ol60oH6_A.png" data-width="602" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*mJhaHQvfyy8c9Ol60oH6_A.png"><figcaption class="imageCaption">Bare Metal</figcaption></figure><p name="86e7" id="86e7" class="graf graf--p graf-after--figure">Users can use this basic service either through the web or through a mobile device. This machine contains the APIs for business logic and a database for data storage. This is where all projects begin and is the easiest to implement.</p><p name="a9e4" id="a9e4" class="graf graf--p graf-after--p">When the concept has been successfully validated, the number of users will start to rise, easily exceeding the limit of what a single machine can handle. At this point, we choose to move the service from a laptop to a professional server in order to continue to verify that the project will continue to grow and not be a flash in the pan. The upgrade of hardware specifications is called vertical scaling, a.k.a. scale-up.</p><p name="82eb" id="82eb" class="graf graf--p graf-after--p">However, even for professional servers, where hardware can be easily upgraded and swapped out, there are limits. The limit here is not only a physical limit, but also a budget limit. For example, a 1TB SSD drive is more than twice as expensive as a 512GB drive. The curve of increasing hardware costs is exponential.</p><p name="a4e0" id="a4e0" class="graf graf--p graf-after--p">Therefore it is necessary to separate the components to make the cost more manageable, especially for the database, which is usually the most costly component.</p><h3 name="f462" id="f462" class="graf graf--h3 graf-after--p">Layered Architecture</h3><figure name="f90e" id="f90e" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*tPii5axH60TmS-tgbpKsyQ.png" data-width="520" data-height="162" src="https://cdn-images-1.medium.com/max/800/1*tPii5axH60TmS-tgbpKsyQ.png"><figcaption class="imageCaption">Layered Architecture</figcaption></figure><p name="9864" id="9864" class="graf graf--p graf-after--figure">When we separate the database from the API, we are able to upgrade their hardware individually. From the original scale-up of a single machine to the scale-up of individual components.</p><p name="06a4" id="06a4" class="graf graf--p graf-after--p">The hardware specifications of the database are usually very advanced, but the API ones are not. The main reason is that as the users grow, the API needs a stronger CPU to handle more traffic, but the rest of the resources are not as urgent. Upgrading CPUs runs into the same problem as hard drives, an exponentially increasing cost curve.</p><p name="b4c0" id="b4c0" class="graf graf--p graf-after--p">In other words, it is much more cost effective to increase the number of CPUs than to increase the size of CPUs. So in order to handle the increased traffic, we usually adopt the strategy of increasing the number of APIs.</p><h3 name="df76" id="df76" class="graf graf--h3 graf-after--p">Horizontal Scaling</h3><figure name="065e" id="065e" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*Mo9aNau-KNDwL8OihumzGQ.png" data-width="846" data-height="460" src="https://cdn-images-1.medium.com/max/800/1*Mo9aNau-KNDwL8OihumzGQ.png"><figcaption class="imageCaption">Horizontal Scaling</figcaption></figure><p name="66a7" id="66a7" class="graf graf--p graf-after--figure">In order to get the number of APIs to scale smoothly, we will need a new role to assign all incoming traffic, called load balancer. The load balancer assigns the traffic based on its algorithm, the common algorithms are RR (Round Robin) and LU (Least Used), but I suggest to go for the simplest one, RR, because the complex algorithm puts an extra load on the load balancer and makes it another kind of bottleneck.</p><p name="8c5e" id="8c5e" class="graf graf--p graf-after--p">When the user’s usage changes, the number of APIs can be dynamically adjusted, for example, the number of APIs is increased when the usage goes up, which is called scale-out, and the opposite is called scale-in.</p><p name="ffb6" id="ffb6" class="graf graf--p graf-after--p">So far, the APIs can be scaled up to handle the increasing traffic, but there is another bottleneck, the database, which can be found in the above diagram.</p><p name="2b80" id="2b80" class="graf graf--p graf-after--p">When usage increases to a certain amount, a single database will not be able to handle it efficiently, resulting in an overall increase in response time. The result is a poor user experience and possibly even a failure of functionality. There are many articles on <a href="https://uxplanet.org/how-page-speed-affects-web-user-experience-83b6d6b1d7d7" data-href="https://uxplanet.org/how-page-speed-affects-web-user-experience-83b6d6b1d7d7" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">how page speed affects web user experience</a>, so I won’t go into detail here.</p><p name="22fc" id="22fc" class="graf graf--p graf-after--p">In order to solve the database bottleneck, we would also like to have horizontal scaling of the database. However, unlike the stateless characteristic of APIs, databases are usually stateful and therefore cannot be simply scale-out.</p><h3 name="7b5c" id="7b5c" class="graf graf--h3 graf-after--p">Read Write Splitting</h3><p name="db55" id="db55" class="graf graf--p graf-after--h3">To enable the database to be scaled horizontally, a common practice is called Read/Write Splitting.</p><figure name="dc9d" id="dc9d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YDcMMhDpJ-_9Bdc41HKCyA.png" data-width="1394" data-height="612" src="https://cdn-images-1.medium.com/max/800/1*YDcMMhDpJ-_9Bdc41HKCyA.png"><figcaption class="imageCaption">Read/Write Splitting</figcaption></figure><p name="c351" id="c351" class="graf graf--p graf-after--figure">First, we keep all writes to the same database entity to maintain the state of the database. Reads, on the other hand, are reads on entities that can scale horizontally. Keeping reads and writes split makes it easier to scale-out the database.</p><p name="dcb3" id="dcb3" class="graf graf--p graf-after--p">When there are data updates, the primary is responsible for replicating the changes to each of the read entities. In this way, the read entities can be scaled out based on usage.</p><p name="d4fb" id="d4fb" class="graf graf--p graf-after--p">This is seen in several common databases, such as MySQL Replication and MongoDB ReplicaSet.</p><p name="36a8" id="36a8" class="graf graf--p graf-after--p">Nevertheless, there is a problem that in order to handle more reads, we have to create more replicas of the database, which is a challenge for the budget. Because of the high hardware specifications of the databases, creating replicas is a very high price to pay.</p><p name="ac81" id="ac81" class="graf graf--p graf-after--p">Is there a way to support the traffic and save cost at the same time? Yes, caching.</p><h3 name="bad3" id="bad3" class="graf graf--h3 graf-after--p">Caching</h3><p name="7c04" id="7c04" class="graf graf--p graf-after--h3">There are several types of caching practices, the common ones are:</p><ol class="postList"><li name="f5ae" id="f5ae" class="graf graf--li graf-after--p">Read-aside cache</li><li name="d9a5" id="d9a5" class="graf graf--li graf-after--li">Content Delivery Network, CDN</li></ol><p name="4590" id="4590" class="graf graf--p graf-after--li">There are advantages and disadvantages to both approaches, which will be briefly analyzed below.</p><h3 name="875b" id="875b" class="graf graf--h3 graf-after--p">Read-aside Cache</h3><figure name="4e17" id="4e17" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*H3N_CRqQsbbBNP28fgt-hQ.png" data-width="912" data-height="488" src="https://cdn-images-1.medium.com/max/800/1*H3N_CRqQsbbBNP28fgt-hQ.png"><figcaption class="imageCaption">Read-aside Cache</figcaption></figure><p name="bac6" id="bac6" class="graf graf--p graf-after--figure">In addition to the original database, we put a cache aside. The whole process of reading is,</p><ol class="postList"><li name="b3e6" id="b3e6" class="graf graf--li graf-after--p">first read the data from the cache.</li><li name="ba36" id="ba36" class="graf graf--li graf-after--li">if the data does not exist in the cache, read it from the database instead.</li><li name="1824" id="1824" class="graf graf--li graf-after--li">then write back to the cache.</li></ol><p name="d5c1" id="d5c1" class="graf graf--p graf-after--li">This is the most common scenario for caching. Depending on the nature of the data, different time to live, TTL will be set in the cached dataset.</p><h3 name="1bfc" id="1bfc" class="graf graf--h3 graf-after--p">CDN</h3><p name="b389" id="b389" class="graf graf--p graf-after--h3">Another caching practice is CDN.</p><figure name="44e9" id="44e9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dMOjwBzvwMlyNzB2TzXrIA.png" data-width="1042" data-height="464" src="https://cdn-images-1.medium.com/max/800/1*dMOjwBzvwMlyNzB2TzXrIA.png"><figcaption class="imageCaption">CDN</figcaption></figure><p name="4d6c" id="4d6c" class="graf graf--p graf-after--figure">Instead of building a cache, a new component is used and placed in front of the load balancer. When any read request comes in, the CDN first determines if there is already cached data based on the configured rules. If there is, the request will be replied directly. On the other hand, the request will follow the original process, and the data will be cached when the response passes through the CDN so that it can be replied directly next time.</p><p name="af04" id="af04" class="graf graf--p graf-after--p graf--trailing">Compared with the read-aside cache, we can see from the diagram there are fewer lines in the CDN. Fewer lines means less complexity in application implementation and easier to achieve. After all, the CDN only needs to configure the rules and the application does not need to be changed at all.</p></div></div></section><section name="ec20" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="f16f" id="f16f" class="graf graf--p graf--leading">One potential problem with caching is the inconsistency of the data. When the data is updated in the database, if the TTL of the cache does not expire, the cached data will not be updated, and then the user may see the inconsistent result.</p><p name="28e4" id="28e4" class="graf graf--p graf-after--p">For read-aside cache, when the API updates the database, it can delete the data in the cache at the same time, so that the next time when it reads in, it can get the latest data. On the other hand, CDNs have to perform invalidation through the APIs provided by each vendor, which is more complicated than read-aside cache in practice.</p><h3 name="cae9" id="cae9" class="graf graf--h3 graf-after--p">Writing Bottlenecks</h3><p name="3e8f" id="3e8f" class="graf graf--p graf-after--h3">Caching and read/write splitting have been effective in handling the increasing amount of read requests, but as you can see from the above diagrams, there is still no effective way to deal with the large number of writes to the database. To solve the write bottleneck, there are many different mechanisms that can be applied, and two common approaches are listed below.</p><ol class="postList"><li name="82c1" id="82c1" class="graf graf--li graf-after--p">Master-master replication</li><li name="e9df" id="e9df" class="graf graf--li graf-after--li">Write through cache</li></ol><p name="2175" id="2175" class="graf graf--p graf-after--li">These two approaches are orthogonal and have completely different practical considerations and are difficult to compare with each other. Then, let’s analyze these two approaches.</p><h3 name="c8e8" id="c8e8" class="graf graf--h3 graf-after--p">Master-master Replication</h3><figure name="1fc7" id="1fc7" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*4843yOgJdM3nknRx80XbPA.png" data-width="1406" data-height="502" src="https://cdn-images-1.medium.com/max/800/1*4843yOgJdM3nknRx80XbPA.png"><figcaption class="imageCaption">Master-master Replication</figcaption></figure><p name="0ed3" id="0ed3" class="graf graf--p graf-after--figure">Although there is a corresponding database entity for each of the APIs in the above diagram, the APIs are not actually locked to a particular entity, depending on the database implementation in use and configuration.</p><p name="f3c7" id="f3c7" class="graf graf--p graf-after--p">Comparing with read-write split, you will find both read and write can be executed on the same database entity. Writes on any database entity will be synchronized to other entities.</p><p name="bec4" id="bec4" class="graf graf--p graf-after--p">A typical example of master-master replication is Cassandra, which is very scalable for writes and can support very large numbers of simultaneous writes. On the other hand, MySQL also supports master-master replication, but one major problem with MySQL master-master replication is its replication is performed asynchronously in the background, which sacrifices MySQL’s most important feature, consistency.</p><p name="2472" id="2472" class="graf graf--p graf-after--p">If master-master replication is needed, PACELC must be taken into account.</p><p name="90f0" id="90f0" class="graf graf--p graf-after--p">Master-master replication is a technique that tolerates partition failure, so you can only choose between consistency and availability. In addition, MySQL can achieve consistent master-master replication with external frameworks, such as Galera Cluster, which in returns, creates long latency.</p><p name="f1ec" id="f1ec" class="graf graf--p graf-after--p">Therefore, when choosing master-master replication to improve write performance, it is important to consider whether the usage scenario is appropriate. If you have to apply such a technique to MySQL, it would be better to switch to another database, e.g. Cassandra.</p><p name="5a99" id="5a99" class="graf graf--p graf-after--p">However, changing the database is a huge effort. Is there a way to solve the write bottleneck without changing the database? Yes, through caching once again.</p><h3 name="70ce" id="70ce" class="graf graf--h3 graf-after--p">Write through Cache</h3><figure name="8545" id="8545" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*PDEjfTJLYXx55BBKb3Nzrw.png" data-width="1070" data-height="460" src="https://cdn-images-1.medium.com/max/800/1*PDEjfTJLYXx55BBKb3Nzrw.png"><figcaption class="imageCaption">Write through Cache</figcaption></figure><p name="3431" id="3431" class="graf graf--p graf-after--figure">Before writing the data into the database by API, write all the data into the cache. After a period of time, the results of the cache are then written into the database in batches. By this way, a large number of write operations can be turned into a small number of batch operations, which can effectively reduce the load on the database.</p><p name="477c" id="477c" class="graf graf--p graf-after--p">The read operation can also be like read-aside cache which reads from the cache first and then the database, so that on the one hand we can get the first update and on the other hand we can reduce the load on the database read.</p><p name="8328" id="8328" class="graf graf--p graf-after--p">Write through cache is a completely different design pattern from master-master replication, which scales the database performance through architectural changes while keeping the original database. Although the application does not need to be modified by changing the database, writing through cache will also create additional complexity.</p><p name="8e9f" id="8e9f" class="graf graf--p graf-after--p">We’ve covered how the entire web service deals with heavy traffic, from scale-up to scale-out, from API to database, but the story doesn’t end here. We have overcome the traffic problem, but when the volume of data in the database is very large, the performance of the database, both in terms of reads and writes, is seriously affected. Given the large volume of data, even a few reads can take up a huge amount of resources, which leads to database performance problems.</p><h3 name="369e" id="369e" class="graf graf--h3 graf-after--p">Sharding</h3><p name="ae7f" id="ae7f" class="graf graf--p graf-after--h3">Since the amount of data is too large for a single database, it is sufficient to spread the data evenly across several database entities, which is the concept of sharding.</p><figure name="28a3" id="28a3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3VbI4_XijMmJ_b-jSZQTDQ.png" data-width="1122" data-height="878" src="https://cdn-images-1.medium.com/max/800/1*3VbI4_XijMmJ_b-jSZQTDQ.png"><figcaption class="imageCaption">Sharding</figcaption></figure><p name="a0a7" id="a0a7" class="graf graf--p graf-after--figure">Although there is only one database in each of the above clusters, all three of them are actually database clusters, and the mentioned read-write split or master-master replication can be applied. Furthermore, even though the APIs correspond to a database cluster individually, it does not mean that the APIs can only access specific clusters. For example, <code class="markup--code markup--p-code">API2</code> can also access <code class="markup--code markup--p-code">Cluster3</code>.</p><p name="9dd0" id="9dd0" class="graf graf--p graf-after--p">Sharding is the technique of dividing a large dataset into several smaller datasets. By using pre-defined indexes such as shard key (MongoDB) or partition key (Cassandra), the data is distributed to the corresponding database entities. Thus, for an application, accessing a specific data will be in a specific database entity, and if the data is spread evenly enough, then the load of individual database entity is <code class="markup--code markup--p-code">1/N</code>, <code class="markup--code markup--p-code">N</code> being the number of clusters.</p><p name="f2cc" id="f2cc" class="graf graf--p graf-after--p">Nevertheless, if the data is not spread out enough and is overly concentrated in one database, the meaning of sharding is lost, which is called a hot spot, and this will not only cause the performance to drop, but also the cost of redundant database. As I mentioned earlier, the cost of a database is high, and it would be very wasteful to have a redundant database.</p><p name="df36" id="df36" class="graf graf--p graf-after--p">Once we have overcome the problem of scaling traffic and data volume, the next challenge is what to do when the task to be performed becomes large enough to affect the performance of the API and the database? The answer is to break up the task.</p><h3 name="5c5f" id="5c5f" class="graf graf--h3 graf-after--p">Messaging</h3><p name="eeae" id="eeae" class="graf graf--p graf-after--h3">When the tasks running by the API become large, the response time of the API will be significantly affected. To reduce the response time of the API for large tasks, the most common approach is to perform synchronous tasks asynchronously, and sometimes even divide the large tasks into several smaller ones.</p><p name="5ec8" id="5ec8" class="graf graf--p graf-after--p">The asynchronous approach has actually been mentioned in many of my previous articles, that is, the Event-Driven Architecture.</p><figure name="09ba" id="09ba" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PoA_dF4nTmON98LB-U909Q.png" data-width="1196" data-height="464" src="https://cdn-images-1.medium.com/max/800/1*PoA_dF4nTmON98LB-U909Q.png"><figcaption class="imageCaption">Messaging</figcaption></figure><p name="bbec" id="bbec" class="graf graf--p graf-after--figure">The API sends tasks to the message queue, which are executed by workers behind the queue. As the number of events grows, the number of workers can be scaled horizontally depending on the number of pending events.</p><p name="9949" id="9949" class="graf graf--p graf-after--p">The details of the event-driven architecture are listed below, so I won’t dive into them in this article.</p><ul class="postList"><li name="c642" id="c642" class="graf graf--li graf-after--p">How to choose a message queue? <a href="https://selectfrom.dev/message-queue-in-redis-9efe0de2c39c" data-href="https://selectfrom.dev/message-queue-in-redis-9efe0de2c39c" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">There are four main aspects</a>.</li><li name="7036" id="7036" class="graf graf--li graf-after--li">What is the event-driven architecture pattern? <a href="https://lazypro.medium.com/design-patterns-of-event-driven-architecture-bf0121cfda7b" data-href="https://lazypro.medium.com/design-patterns-of-event-driven-architecture-bf0121cfda7b" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Design pattern, Part 1</a> and <a href="https://lazypro.medium.com/design-patterns-of-event-driven-architecture-part-2-ea4296dc58d" data-href="https://lazypro.medium.com/design-patterns-of-event-driven-architecture-part-2-ea4296dc58d" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Design pattern, Part 2</a>.</li><li name="8ea9" id="8ea9" class="graf graf--li graf-after--li">How to implement with minimal effort? <a href="https://betterprogramming.pub/implement-event-driven-architecture-with-minimal-effort-182c3bbe5524" data-href="https://betterprogramming.pub/implement-event-driven-architecture-with-minimal-effort-182c3bbe5524" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Solution</a>.</li></ul><p name="9036" id="9036" class="graf graf--p graf-after--li">Finally, when the project has been successful, the users are around the world. If the data center is located in the same place, then for areas that are physically far away, the delay will be noticeable, resulting in a decrease in user acceptance. So, how do we address the scalability of users?</p><h3 name="b5c9" id="b5c9" class="graf graf--h3 graf-after--p">Edge Computing</h3><p name="9255" id="9255" class="graf graf--p graf-after--h3">Edge means to arrange the system as close to the user as possible for a good user experience.</p><figure name="f930" id="f930" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZfFrkurRSLzkUctXTj8msw.png" data-width="1400" data-height="956" src="https://cdn-images-1.medium.com/max/800/1*ZfFrkurRSLzkUctXTj8msw.png"><figcaption class="imageCaption">Edge Computing</figcaption></figure><p name="bd40" id="bd40" class="graf graf--p graf-after--figure">Therefore, for three distant regions, we can set up three different data centers to provide better efficiency for users in each region.</p><p name="bc90" id="bc90" class="graf graf--p graf-after--p">Nevertheless, when we need to perform data analysis, we will always need data from all three regions. From a data analysis perspective, we need one logically unified database, not three physically independent databases.</p><p name="070c" id="070c" class="graf graf--p graf-after--p">In other words, how to get the database as close to the user as possible and still have a unified entry point?</p><ul class="postList"><li name="d573" id="d573" class="graf graf--li graf-after--p">DB shards: This is a relatively simple approach to practice. Just use the region as the shard key and create database shards in each region. Then, there will be a unified entry point, which in the case of MongoDB is <code class="markup--code markup--li-code">mongos</code>.</li><li name="ea0d" id="ea0d" class="graf graf--li graf-after--li">DB master-master replication: master-master replication also allows different database entities to be placed in different regions, but because of the physical distance, the replication is not very efficient, so the synchronization rate is a potential problem.</li><li name="4025" id="4025" class="graf graf--li graf-after--li">Data ETL: Data is extracted from various databases and transformed and loaded into a unified data store. This is the most common way for data analysts to do this without changing the database structure of the original application, as well as to pre-process the data and even choose their own familiar data storage.</li></ul><h3 name="a05c" id="a05c" class="graf graf--h3 graf-after--li">Conclusion</h3><p name="07bd" id="07bd" class="graf graf--p graf-after--h3">This article analyzes how to scale web services from various perspectives. From the beginning of the project, the API is horizontally scaled to handle the large number of incoming requests until the database becomes a bottleneck. To solve the database bottleneck, no simple horizontal scaling can be used, so techniques such as read-write splitting or caching are used to reduce the load on the database. However, if the dataset is very large, we still need to adopt sharding and other methods to separate the dataset. Finally, if the users are worldwide, it is necessary to establish different data centers to physically distribute the workload.</p><p name="54b8" id="54b8" class="graf graf--p graf-after--p">However, while this article is focused on explaining infrastructure-related topics, there is another kind of service scaling that is actually very important, and that is the scaling of feature requirements.</p><p name="7a6e" id="7a6e" class="graf graf--p graf-after--p">As a project moves towards success, more and more feature requests will be made, so how to quickly respond to feature scaling? The most common technology used nowadays is microservices, but microservices also have problems which must be faced, and in my previous article, I introduced what are the aspects of designing a microservice architecture that are worth considering.</p><ul class="postList"><li name="a560" id="a560" class="graf graf--li graf-after--p"><a href="https://medium.com/interviewnoodle/original-sin-of-microservices-part-1-90461ddcefb" data-href="https://medium.com/interviewnoodle/original-sin-of-microservices-part-1-90461ddcefb" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Original Sin of Microservices, Part 1</a></li><li name="baf4" id="baf4" class="graf graf--li graf-after--li"><a href="https://lazypro.medium.com/original-sin-of-microservices-part-2-8856c0e8426d" data-href="https://lazypro.medium.com/original-sin-of-microservices-part-2-8856c0e8426d" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Original Sin of Microservices, Part 2</a></li></ul><p name="2293" id="2293" class="graf graf--p graf-after--li graf--trailing">This article serves as an internal training for my team members, because the target is junior engineers, so there is not too much detail on each topic. If you are interested in any of the topics, please let me know and I will analyze those techniques in depth.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/b391557a1134"><time class="dt-published" datetime="2022-05-30T01:50:36.124Z">May 30, 2022</time></a>.</p><p><a href="https://medium.com/@lazypro/scaling-web-service-b391557a1134" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>