<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Orchestrating DBT with Airflow</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Orchestrating DBT with Airflow</h1>
</header>
<section data-field="subtitle" class="p-summary">
Make an easy management data pipeline
</section>
<section data-field="body" class="e-content">
<section name="e6a5" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7d67" id="7d67" class="graf graf--h3 graf--leading graf--title">Orchestrating DBT with Airflow</h3><h4 name="10e3" id="10e3" class="graf graf--h4 graf-after--h3 graf--subtitle">Make an easy management data pipeline</h4><figure name="c8da" id="c8da" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*wSiyV-t8Rv4TvWtn" data-width="1000" data-height="667" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*wSiyV-t8Rv4TvWtn"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@larisabirta" data-href="https://unsplash.com/@larisabirta" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Larisa Birta</a> on <a href="https://unsplash.com/photos/slbOcNlWNHA" data-href="https://unsplash.com/photos/slbOcNlWNHA" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Unsplash</a></figcaption></figure><p name="1a8a" id="1a8a" class="graf graf--p graf-after--figure"><a href="https://airflow.apache.org/" data-href="https://airflow.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Airflow</a> is one of the great orchestrator today. It is a highly scalable and available framework for orchestrating, and more importantly, it is developed in Python. Therefore, developers can develop in Python based on the various operators provided by Airflow. In addition, the Airflow community is quite active, and many operators are constantly added to the ecosystem.</p><p name="f476" id="f476" class="graf graf--p graf-after--p">These advantages make Airflow the preferred choice for data engineers working on ETL or ELT, although there are many other frameworks for orchestrating workflows, such as <a href="https://argoproj.github.io/argo-workflows/" data-href="https://argoproj.github.io/argo-workflows/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Argo workflow</a>, but for engineers who rely heavily on Python development, Airflow is much easier to harness and maintain.</p><p name="58c2" id="58c2" class="graf graf--p graf-after--p">But Airflow is not without drawbacks, one of the biggest problems is it is a monolith. When workflows are continuously added, this monolith will sooner or later become a big ball of mud. Another problem is that Airflow is a distributed framework and it is not easy to verify a complete workflow in local development. There are many tools that improve on Airflow’s shortcomings, such as <a href="https://dagster.io/" data-href="https://dagster.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dagster</a>, but the fact of the monolith remains unresolved.</p><p name="450f" id="450f" class="graf graf--p graf-after--p">Although Airflow has been widely used in a diversity of data engineering domains, the role of the data domain has been further differentiated as the domain has become increasingly popular. In addition to the traditional Data Engineer, Data Analyst and ML Engineer, a new role has recently become more common and is called Analytical Engineer.</p><p name="9842" id="9842" class="graf graf--p graf-after--p">Let’s briefly describe the responsibilities and capabilities of these roles.</p><ul class="postList"><li name="462f" id="462f" class="graf graf--li graf-after--p">Data Engineers: are responsible for developing and managing the entire data pipeline lifecycle, i.e. they are the main developers of ETL and ELT. They are mainly proficient in distributed architectures, programming (mainly Python), and data storage and its declarative language (mainly SQL).</li><li name="f211" id="f211" class="graf graf--li graf-after--li">Data Analyst: End-user of data, using already structured data for analysis in various business situations. Very good at using SQL and various data visualization tools.</li><li name="6d34" id="6d34" class="graf graf--li graf-after--li">ML Engineer: This role is interesting and has a different position in each organization, but in general, work related to machine learning, such as tagging data, data cleansing, data normalization will be involved, so there are opportunities to develop ETL/ELT, but the main responsibility is still the framework and model of machine learning, of course, is mainly in Python.</li></ul><p name="d687" id="d687" class="graf graf--p graf-after--li">So, what is the position of an Analytical Engineer?</p><p name="dd7d" id="dd7d" class="graf graf--p graf-after--p">They are also responsible for managing the lifecycle of the data pipeline, but the biggest difference between them and data engineers is their lack of the concept of distributed architecture and the ability to develop programs.</p><p name="e2f1" id="e2f1" class="graf graf--p graf-after--p">Well, how to develop a data pipeline without programming? Thanks to the various tools in the data ecosystem, especially <a href="https://www.getdbt.com/" data-href="https://www.getdbt.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dbt</a>, even if you don’t know programming, you can still implement data pipelines as long as you are familiar with SQL.</p><p name="51ec" id="51ec" class="graf graf--p graf-after--p">The emergence of such a role brings a new perspective into the data world.</p><p name="0709" id="0709" class="graf graf--p graf-after--p">Traditionally, in an Airflow-based data pipeline, although most of the business logic is made up of SQL, there is a lot of code that must surround the SQL in order to make it work. For example</p><ol class="postList"><li name="b066" id="b066" class="graf graf--li graf-after--p">the declaration of the DAG.</li><li name="1ce6" id="1ce6" class="graf graf--li graf-after--li">database connections, including credentials, connection pools, etc.</li><li name="e488" id="e488" class="graf graf--li graf-after--li">implementation of SQL calls through PythonOperator or the corresponding database operator.</li><li name="751b" id="751b" class="graf graf--li graf-after--li">defining the upstream and downstream dependencies of the DAG</li></ol><p name="9e23" id="9e23" class="graf graf--p graf-after--li">In Airflow, all these must be developed in Python, but it is undoubtedly a big challenge for those who do not know how to program.</p><h3 name="3428" id="3428" class="graf graf--h3 graf-after--p">Decomposing the data pipeline</h3><p name="fe2b" id="fe2b" class="graf graf--p graf-after--h3">It’s time to decompose Airflow, but before we talk about decomposition, let’s talk about why we need Airflow still instead of replacing it.</p><p name="cdba" id="cdba" class="graf graf--p graf-after--p">Airflow is a competent orchestrator, i.e., workflow manager. Have you wondered before</p><blockquote name="ee27" id="ee27" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">What exactly is an orchestrator?</em></blockquote><p name="773b" id="773b" class="graf graf--p graf-after--blockquote">Let me illustrate with a simple example.</p><figure name="dc23" id="dc23" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UjXuA67G7kvYQLNqJwWSlw.png" data-width="686" data-height="500" src="https://cdn-images-1.medium.com/max/800/1*UjXuA67G7kvYQLNqJwWSlw.png"></figure><p name="1cee" id="1cee" class="graf graf--p graf-after--figure">From the above diagram, this is a typical ETL, the following tasks cannot be executed until the previous task is done, and there is a dependency relationship between all tasks, which is a workflow. The purpose of the orchestrator is to ensure that each task can follow the instructions, one after the other.</p><p name="6f27" id="6f27" class="graf graf--p graf-after--p">What if the previous job fails? Then it automatically retries. The orchestrator will make sure that the entire workflow follows the direction you have planned. If it does fail, it can also send a notification to inform the developer something wrong.</p><p name="1fa1" id="1fa1" class="graf graf--p graf-after--p">From the above description, we know it cannot be solved by replacing Airflow, although it is feasible to move all the business logic from Airflow to dbt. If we put all the processing into dbt, we will immediately encounter a problem: what should we do if something goes wrong after executing <code class="markup--code markup--p-code">dbt run</code>?</p><p name="cfff" id="cfff" class="graf graf--p graf-after--p">Do we execute <code class="markup--code markup--p-code">dbt run</code> again?</p><p name="cd48" id="cd48" class="graf graf--p graf-after--p">A huge package of retries has several issues as follows.</p><ul class="postList"><li name="4ab1" id="4ab1" class="graf graf--li graf-after--p">It can take a long time</li><li name="3fbd" id="3fbd" class="graf graf--li graf-after--li">The development must be done with extra care, all models must be idempotence.</li><li name="7d63" id="7d63" class="graf graf--li graf-after--li">The cost of accessing the database and the cost of network transmission are not low when the volume of data is high.</li></ul><p name="ad1c" id="ad1c" class="graf graf--p graf-after--li">Therefore, decomposing Airflow seems to be a more reliable solution.</p><p name="0037" id="0037" class="graf graf--p graf-after--p">The tasks can be delivered to dbt are done by dbt, but the whole workflow is still under Airflow’s control.</p><p name="3e01" id="3e01" class="graf graf--p graf-after--p">Oops, that’s a loop, isn’t it? We still need Airflow and we still need programming. No, not really, we can still do complete development on dbt, we just need a mechanism to enable Airflow to understand the model composition and relationships within dbt, and then Airflow can trigger dbt.</p><h3 name="16ea" id="16ea" class="graf graf--h3 graf-after--p">Back to the topic</h3><p name="d7d9" id="d7d9" class="graf graf--p graf-after--h3">What we want to do is exactly as the title is orchestrating DBT with Airflow.</p><p name="4103" id="4103" class="graf graf--p graf-after--p">In fact, we have surveyed many solutions in this process, all of which are not ideal. But finally, we found a relatively reliable solution.</p><p name="58b0" id="58b0" class="graf graf--p graf-after--p">Here is the reference we found, which is a trilogy and similar to the problem we are trying to solve.</p><ul class="postList"><li name="abfb" id="abfb" class="graf graf--li graf-after--p"><a href="https://www.astronomer.io/blog/airflow-dbt-1" data-href="https://www.astronomer.io/blog/airflow-dbt-1" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Building a Scalable Analytics Architecture With Airflow and dbt</a></li><li name="905f" id="905f" class="graf graf--li graf-after--li"><a href="https://www.astronomer.io/blog/airflow-dbt-2/" data-href="https://www.astronomer.io/blog/airflow-dbt-2/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Building a Scalable Analytics Architecture With Airflow and dbt: Part 2</a></li><li name="bd25" id="bd25" class="graf graf--li graf-after--li"><a href="https://www.astronomer.io/blog/airflow-dbt-3/" data-href="https://www.astronomer.io/blog/airflow-dbt-3/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Building a Scalable Analytics Architecture With Airflow and dbt: Part 3</a></li></ul><p name="a51e" id="a51e" class="graf graf--p graf-after--li">Let me summarize briefly.</p><p name="db30" id="db30" class="graf graf--p graf-after--p">First, we still use dbt for complete development, which also solves the problem that Airflow is not good at local development.</p><p name="a691" id="a691" class="graf graf--p graf-after--p">Then, the CI/CD pipeline is triggered to execute <code class="markup--code markup--p-code">dbt compile</code> when we commit the dbt-related code into the version control system, in order to get the artifact <code class="markup--code markup--p-code">manifest.json</code> under the <code class="markup--code markup--p-code">target</code> folder.</p><p name="cfe5" id="cfe5" class="graf graf--p graf-after--p">With <code class="markup--code markup--p-code">manifest.json</code>, we can get the dbt model dependencies by reading the file in Airflow, then we only need to write the basic conversion framework, and then we can call the dbt corresponding models through Airflow’s BashOperator.</p><p name="191d" id="191d" class="graf graf--p graf-after--p">The whole development process can be dbt-based, and take full advantage of Airflow’s workflow management capabilities.</p><h3 name="8c90" id="8c90" class="graf graf--h3 graf-after--p">Conslusion</h3><p name="490d" id="490d" class="graf graf--p graf-after--h3">Decomposing monoliths is the trend of complex software development nowadays, just like the rise of microservices architecture, there will be a need for decomposing in the big data area.</p><p name="d83e" id="d83e" class="graf graf--p graf-after--p">However, in the big data area, decomposition is not just a clear cut of each domain based on domain-driven development. After all, when doing data analysis, we always want to have a centralized data warehouse.</p><p name="a6ac" id="a6ac" class="graf graf--p graf-after--p">Therefore, the data engineer’s approach to decomposition will be different from the software engineer’s. In this case, although we are decomposing Airflow, we still have a new dbt as a monolith, except Airflow calls dbt by model.</p><p name="910e" id="910e" class="graf graf--p graf-after--p">The decomposition is done at the level of the model and the execution period, which is a new attempt. I feel this concept can also be introduced into software development.</p><p name="0aaa" id="0aaa" class="graf graf--p graf-after--p graf--trailing">There is also a decomposition approach that is introduced from microservices architecture into the big data area, i.e. data mesh. But this article is already too long, let’s save the data mesh for the future.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@lazypro" class="p-author h-card">Chunting Wu</a> on <a href="https://medium.com/p/72c49d4be62"><time class="dt-published" datetime="2023-02-20T02:02:46.995Z">February 20, 2023</time></a>.</p><p><a href="https://medium.com/@lazypro/orchestrating-dbt-with-airflow-72c49d4be62" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 21, 2024.</p></footer></article></body></html>